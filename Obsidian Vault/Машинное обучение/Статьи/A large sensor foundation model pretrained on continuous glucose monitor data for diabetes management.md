

### Наборы данных (Datasets)

В исследовании использовались два основных набора данных: один для предварительного обучения (Welldoc) и один для бенчмаркинга и тестирования на обобщаемость (OhioT1DM).

1.  **Набор данных Welldoc (для предварительного обучения CGM-LSM):**
    *   **Объем:** Изначально содержал 21 215 912 записей НМГ (непрерывного мониторинга глюкозы) от 617 пациентов. После фильтрации осталось **15 961 183 валидных записи НМГ от 592 человек**.
    *   **Типы диабета:** 291 пациент с диабетом 1-го типа (T1D) и 301 пациент с диабетом 2-го типа (T2D).
        *   T1D: 7 788 836 записей.
        *   T2D: 8 172 347 записей.
    *   **Демография:**
        *   **Возраст:** Записи распределены по возрастным группам: 18-39 лет, 40-64 года, 65+.
            *   T1D: 2 901 601 запись (18-39 лет), 3 249 786 записей (40-64 года), 1 637 449 записей (65+).
            *   T2D: 524 354 записи (18-39 лет), 4 790 337 записей (40-64 года), 2 857 656 записей (65+).
        *   **Пол:** 52,2% мужчин (309 человек) и 47,8% женщин (283 человека).
            *   Женщины T1D: 4 150 047 записей.
            *   Женщины T2D: 2 983 547 записей.
    *   **Характеристики данных:** Это проприетарный набор данных. Для деидентификации использовался метод "сдвига случайного дня" для временных меток, что сохраняет относительные паттерны и временную когерентность, но обеспечивает анонимность.
    *   **Критерии фильтрации экземпляров (instances):** Для каждой "инстанции" (комбинация пациента и времени наблюдения) требовалось:
        *   Наличие 288 показаний НМГ за 24 часа до времени наблюдения (для входных данных).
        *   Наличие 24 показаний НМГ за 2 часа после времени наблюдения (для выходных данных/прогноза).
        *   Модовая скорость изменения НМГ должна быть ниже 40% как для входных, так и для выходных данных, чтобы избежать шума и ошибок измерения.
    *   **Значимость:** Набор данных Welldoc значительно больше и разнообразнее по типам пациентов, чем предыдущие эталонные данные, что позволило модели изучить более широкие закономерности глюкозы.

2.  **Набор данных OhioT1DM (для бенчмаркинга):**
    *   **Объем:** Публично доступный набор данных. Изначально включал 166 532 записи НМГ от 12 пациентов с диабетом 1-го типа. После применения тех же критериев фильтрации, что и для данных Welldoc, осталось **58 414 валидных записей от 12 пациентов T1D**.
    *   **Назначение:** Использовался для сравнения производительности CGM-LSM с базовыми моделями и как второй "задержанный" тестовый набор для проверки обобщаемости без обучения.
    *   **Характеристики данных:** Не содержит демографической информации о пациентах.

### Методы обучения (Training Methods)

Предварительное обучение CGM-LSM основывалось на парадигме больших языковых моделей.

1.  **Определение задачи предварительного обучения:**
    *   Последовательность из 312 значений НМГ с временными метками ($s_1, s_2, s_3, \ldots, s_n$) представляла собой данные за 26-часовой период (24 часа до фокусной даты/времени и 2 часа после). Эта последовательность имеет естественный последовательный порядок, аналогичный естественному языку.
    *   Модель обучалась авторегрессивным методом для предсказания следующего значения глюкозы в последовательности данных НМГ.
    *   Математически это описывается как максимизация правдоподобия значения глюкозы $s_i$ при заданных предыдущих значениях глюкозы $s_1, \ldots, s_{i-1}$.
    *   Целевая функция для обучения модели CGM-LSM формализована следующим образом:
        $$L(\theta) = \sum_{i=1}^{n} \log p_{\theta}(s_i | s_1, \ldots, s_{i-1}) \quad (1)$$
        где $\theta$ представляет параметры модели, $n$ — длина входной последовательности, а $p_{\theta}(s_i | s_1, \ldots, s_{i-1})$ — вероятность значения глюкозы $s_i$, обусловленная предыдущей последовательностью токенов, вычисленная моделью.

2.  **Представление входных данных:**
    *   Токены, используемые в CGM-LSM для предсказания глюкозы, были самими значениями глюкозы.
    *   Для простоты и без потери общности показания глюкозы напрямую использовались как токены; например, уровень глюкозы 153 становился токеном "153".
    *   В общей сложности в словаре было **17 специальных токенов и 400 токенов значений глюкозы**, поскольку максимальное значение глюкозы, измеряемое устройством НМГ, составляет 400.

3.  **Структура модели CGM-LSM:**
    *   CGM-LSM использует **архитектуру на основе декодера Transformer**, которая широко используется в сериях моделей GPT.
    *   В отличие от традиционных рекуррентных нейронных сетей, этот подход полностью полагается на механизмы самовнимания для генерации последовательностей. Такая архитектура способствует более параллельным вычислениям и эффективно захватывает долгосрочные зависимости в данных.
    *   В декодер-ориентированном трансформере модель состоит исключительно из стека блоков декодера, которые обрабатывают входную последовательность для генерации одного токена значения глюкозы за раз. Каждый блок декодера включает два основных компонента: механизм самовнимания с несколькими головками (multi-head self-attention) и позиционно-полносвязную сеть прямого распространения (position-wise fully connected feed-forward network).
    *   Вход в каждый слой декодера изначально преобразуется в три вектора — запроса (Q), ключа (K) и значения (V) — посредством линейной проекции встраиваний входных токенов.
    *   Механизм самовнимания в декодере математически представлен как:
        $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \quad (3)$$
        где $d_k$ — размерность ключевых векторов. Этот механизм позволяет каждой позиции в декодере обращаться ко всем позициям вплоть до текущей позиции в предыдущем слое, что критически важно для авторегрессивных моделей, где предсказание следующего токена может зависеть только от известных предыдущих токенов, сохраняя причинность в генерации последовательности.
    *   Нормализация и остаточные соединения используются вокруг каждого из этих подслоев (самовнимание и сети прямого распространения) для стабилизации процесса обучения. Выход каждого подслоя можно описать как:
        $$\text{LayerNorm}(x + \text{Sublayer}(x)) \quad (4)$$
        где $\text{Sublayer}(x)$ — функция, реализованная самим подслоем (самовнимание или сеть прямого распространения), а $x$ — вход подслоя.
    *   Наконец, после прохождения через серию блоков декодера, выход проецируется на вектор размером со словарь с использованием линейного преобразования, за которым следует слой softmax для получения распределения вероятностей по возможным следующим токенам:
        $$p(s_{i+1} | s_1, \ldots, s_i) = \text{softmax}(h_i W) \quad (5)$$
        где $h_i$ — выход последнего слоя в позиции $i$, а $W$ — весовая матрица.

4.  **Определение задачи прогнозирования/генерации:**
    *   После обучения CGM-LSM может использоваться для генерации новых последовательностей глюкозы. Процесс генерации CGM-LSM аналогичен моделям GPT. Он включает итеративное семплирование нового сгенерированного значения глюкозы (токена) из распределения вероятностей возможных следующих значений глюкозы (токенов), обусловленного последовательностью входных значений глюкозы (токенов).
    *   Математически это представлено как:
        $$p(s_{i+1} | s_1, \ldots, s_i; \theta) = \text{softmax}(h_i W) \quad (2)$$
        где $\theta$ обозначает параметры модели, $h_i$ — скрытое состояние, полученное из блоков трансформера, а $W$ — матрица проекции выхода.
    *   Генерация начинается с начальной последовательности, т.е. с предшествующей 24-часовой последовательности данных НМГ.
    *   Для данного приложения использовался **жадный метод (greedy method)**, который выбирает значение глюкозы с наивысшей вероятностью. Предсказанное значение глюкозы затем рассматривается как реальное значение и добавляется к исходной последовательности для следующего предсказания. Процесс повторяется до достижения максимальной длины, т.е. 24 значений глюкозы за следующие 2 часа.

5.  **Функция потерь:**
    *   Поскольку значение глюкозы рассматривалось как уникальный токен, для задачи предсказания следующего значения глюкозы использовалась **кросс-энтропия**.
    *   Функция softmax играет ключевую роль в преобразовании логитов — необработанных выходных оценок для каждого значения глюкозы в словаре — в распределение вероятностей по всем потенциальным следующим значениям глюкозы.
    *   Для логитов $z_t$ для возможных следующих значений глюкозы в позиции $t+1$, функция softmax вычисляет вероятность каждого токена $x_t$ быть следующим в последовательности следующим образом:
        $$p_{\theta}(s_{i+1} | s_1, \ldots, s_t) = \frac{\exp(z_{t, s_t})}{\sum_{k=1}^{V} \exp(z_{t,k})} \quad (6)$$
        где $V$ — размер словаря, а $z_{t,k}$ представляет логит, соответствующий $k$-му токену словаря.
    *   Потери кросс-энтропии для предсказания одного токена:
        $$\text{Loss} = -\log p_{\theta}(s_{t+1} | s_1, \ldots, s_t) \quad (7)$$
    *   Общие потери для пакета последовательностей, которые агрегируют отдельные потери токенов:
        $$\text{Total Loss} = -\sum_{i=1}^{N} \sum_{t=1}^{T_i} \log \left( \frac{\exp(z_{i,t-1,s_{i,t}})}{\sum_{k=1}^{V} \exp(z_{i,t-1,k})} \right) \quad (8)$$
        Минимизация этих общих потерь во время предварительного обучения позволяет модели CGM-LSM эффективно настраивать свой набор параметров $\theta$, улучшая ее возможности в моделировании языка и предсказании следующего токена.

6.  **Детали реализации CGM-LSM:**
    *   **Основа:** Модель GPT-2, реализованная с помощью библиотеки Hugging Face.
    *   **Параметры модели:** Размер встраивания (embedding size) 768, количество мульти-головок (multi-heads) 12, количество слоев (layers) 12.
    *   **Оптимизатор:** AdamW с $\beta_1 = 0.9$ и $\beta_2 = 0.999$.
    *   **Гиперпараметры:** Скорость обучения (learning rate) 0.00005, $\epsilon$ для нормализации слоя 0.00001, коэффициент отсева (dropout rate) 0.1.
    *   **Пакетный размер (Batch size):** 256.
    *   **Оборудование и эпохи:** Обучение проводилось на одном GPU A100 80GB в течение десяти эпох. Модель на конец третьей эпохи была выбрана как окончательная из-за прироста потерь на валидационных наборах.
    *   **Даунсэмплинг:** Применялся случайный 10% даунсэмплинг наборов данных для снижения переобучения и повышения вычислительной эффективности.

### Валидация и оценка (Validation and Evaluation Strategy)

Для обеспечения надежной оценки производительности модели в терминах точности и надежности использовалась строгая методология разделения данных и несколько метрик.

1.  **Разделение данных (для Welldoc dataset):**
    *   **Тестовый набор "Held-Out" (10% пациентов):** Эти пациенты были полностью исключены из фазы обучения и зарезервированы исключительно для оценки. Цель: проверка **обобщаемости на совершенно новых, ранее не виденных пациентах при включении в систему**.
    *   **Пациенты "Held-In" (оставшиеся 90%):** Данные этих пациентов были разделены следующим образом:
        *   **Временной тестовый набор "Temporal Test Set" (10% инстансов):** Последние 10% инстансов каждого пациента, отсортированные хронологически. Цель: оценка производительности модели на **невиданных будущих периодах**.
        *   **Внутренний тестовый набор "Internal Test Set" (10% инстансов):** Случайные 10% оставшихся инстансов.
        *   **Обучающий набор "Training Dataset" (80% инстансов):** Оставшиеся данные для обучения модели.
    *   **Обоснование:** Такое разделение позволяет оценить модель как на невиданных временных периодах, так и на невиданных пациентах, повышая надежность оценки ее прогностических возможностей.

2.  **Оценка на OhioT1DM:** Этот набор данных использовался как **второй "задержанный" тестовый набор**, что обеспечило строгую проверку обобщаемости без обучения (zero-shot generalization) по сравнению с базовыми моделями.

3.  **Метрики оценки производительности:**
    *   **Среднеквадратичная ошибка (Root Mean Squared Error, rMSE):** Надежная метрика для количественной оценки средней величины ошибки между предсказанными и фактическими значениями. Вычисляется по формуле:
        $$\text{rMSE} = \sqrt{\frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)^2} \quad (9)$$
        где $y_i$ — фактические значения, $\hat{y}_i$ — предсказанные значения, а $n$ — количество экземпляров.
    *   **Средняя абсолютная ошибка (Mean Absolute Error, MAE):** Измеряет среднюю величину ошибок без учета их направления. Вычисляется как:
        $$\text{MAE} = \frac{1}{n}\sum_{i=1}^{n} |y_i - \hat{y}_i| \quad (10)$$
    *   **MAE с допуском (MAE with Tolerance, MAE(T) на уровне 10 единиц):** Измеряет долю абсолютных ошибок, попадающих в заданный уровень допуска (здесь 10 мг/дл). Вычисляется как:
        $$\text{MAE}(\tau) = \frac{1}{n}\sum_{i=1}^{n} \min(\max(0, |y_i - \hat{y}_i| - \tau), 1) \quad (11)$$
    *   **Точность по областям глюкозы (Glucose Region Accuracy):** Оценивает точность прогнозов путем категоризации непрерывных значений глюкозы в предопределенные регионы (очень низкий <54 мг/дл, низкий 54-69 мг/дл, время в диапазоне 70-180 мг/дл, высокий 181-250 мг/дл, очень высокий >250 мг/дл) и сравнения этих категориальных прогнозов с фактическими. Точность затем вычисляется по формуле:
        $$\text{Accuracy} = \frac{\text{Количество правильных предсказаний}}{\text{Общее количество предсказаний}} \quad (12)$$
    *   Эти метрики в совокупности обеспечивают всесторонний взгляд на производительность модели, учитывая как величину ошибок, так и их клиническую значимость.

4.  **Оценка базовых моделей:** Для установления базовой производительности прогнозирования глюкозы на наборе данных OhioT1DM были оценены различные модели прогнозирования временных рядов, включая:
    *   Классические рекуррентные нейронные сети: **Vanilla RNN, GRU, LSTM**.
    *   Усовершенствованные архитектуры на основе трансформеров: **Standard Transformer, Informer, Autoformer**.
    *   Для эффективной реализации этих моделей использовалась библиотека **Nixtla NeuralForecast**.
    *   **Оптимизация гиперпараметров:** Проводилась с использованием фреймворка **Optuna**, который динамически масштабирует этапы обучения в зависимости от размера набора данных и конфигурации пакета. Это позволяло эффективно исследовать пространство гиперпараметров, обеспечивая при этом справедливое сравнение моделей с различными вычислительными требованиями. Каждая модель проходила тщательную оптимизацию с 10-20 попытками для определения оптимальных конфигураций скрытых размерностей, количества слоев, скоростей обучения и коэффициентов отсева. Модели обучались на 289 (24-часовых) входных данных для прогнозирования 24 (2-часовых) будущих значений. Все эксперименты проводились на GPU RTX 4090.

Эти аспекты демонстрируют тщательность подхода к разработке и оценке модели CGM-LSM, что позволило достичь высокой точности и надежности в прогнозировании уровня глюкозы.