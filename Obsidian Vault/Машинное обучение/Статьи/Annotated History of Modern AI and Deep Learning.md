### Юрген Шмидхубер
#### **Введение**
Статья представляет собой хронологический обзор ключевых событий в истории искусственного интеллекта (ИИ) и глубокого обучения, начиная с XVII века и заканчивая современными достижениями. Автор, Юрген Шмидхубер, подчеркивает, что многие фундаментальные концепции ИИ были разработаны задолго до бума нейросетей в 2010-х годах, и акцентирует внимание на вкладе ученых, чьи работы часто оставались незамеченными.

**Машинное обучение (ML)** — это наука о распределении кредитов: нахождение паттернов в наблюдениях, которые предсказывают последствия действий и помогают улучшить будущие результаты. Распределение кредитов нужно не только для понимания человеком того, как устроен мир, но и для ученых, например, историков, которые интерпретируют настоящее, исходя из прошлых событий.

___

#### **Основные разделы и ключевые моменты**

1. **1676: Цепное правило для обратного распространения ошибки**
   - Готфрид Лейбниц опубликовал цепное правило дифференциального исчисления, которое стало основой для алгоритма обратного распространения ошибки (backpropagation) в глубоком обучении.
   - Это правило позволяет эффективно вычислять градиенты в сложных нейронных сетях.

2. **Около 1800: Первая нейронная сеть / линейная регрессия**
   - Адриен-Мари Лежандр и Карл Фридрих Гаусс разработали метод наименьших квадратов (линейную регрессию), который математически эквивалентен простейшей нейронной сети с одним слоем.
   - Пример: Гаусс использовал этот метод для предсказания местоположения карликовой планеты Церера.

3. **1920-1925: Первая рекуррентная архитектура**
   - Эрнст Изинг и Вильгельм Ленц предложили модель Изинга — первую рекуррентную нейронную сеть (RNN) без обучения.
   - Эта модель легла в основу более поздних RNN, таких как сети Хопфилда.

4. **1958: Многослойный перцептрон (без глубокого обучения)**
   - Фрэнк Розенблатт разработал многослойный перцептрон (MLP), но обучение проводилось только для последнего слоя.
   - Это не было глубоким обучением в современном понимании, но стало предшественником современных архитектур.

5. **1965: Первое глубокое обучение**
   - Алексей Ивахненко и Валентин Лапа создали первые алгоритмы для обучения глубоких MLP с произвольным количеством скрытых слоев.
   - Их метод включал поэтапное обучение слоёв и регуляризацию.

6. **1967-1968: Глубокое обучение методом стохастического градиентного спуска (SGD)**
   - Шун-Ити Амари предложил использовать SGD для обучения многослойных сетей, что позволило эффективно обучать внутренние представления данных.

7. **1970: Обратное распространение ошибки**
   - Сеппо Линнайнмаа первым опубликовал алгоритм обратного распространения (backpropagation), хотя его применение к нейронным сетям было предложено позже Полом Вербосом (1982).
   - В 1960-х годах аналогичные идеи разрабатывались в теории управления (Генри Келли, Артур Брайсон).

8. **1979: Первая сверточная нейронная сеть (CNN)**
   - Кунихико Фукусима предложил архитектуру Neocognitron — первую CNN с чередующимися сверточными и субдискретизирующими слоями.
   - В 1969 году он также ввел Rectified Linear Units (ReLU), которые сейчас широко используются.

9. **1990-1991: Прорывные идеи**
   - **Генеративно-состязательные сети (GAN):** Шмидхубер предложил принцип "искусственного любопытства", где генератор и предсказатель соревнуются в минимаксной игре (1990).
   - **Трансформеры с линейным self-attention:** Были представлены "быстрые весовые программисты" (Fast Weight Programmers) — аналоги современных трансформеров (1991).
   - **Самообучение с учителем:** Нейронные сети научились создавать иерархические представления данных через предобучение без учителя (unsupervised pre-training).
   - **Долгая краткосрочная память (LSTM):** Сепп Хохрайтер и Шмидхубер разработали LSTM для решения проблемы исчезающих градиентов (1991).

10. **2010-е: Практические успехи**
    - Глубокие CNN (например, DanNet) и LSTM стали основой для современных приложений: распознавания речи, машинного перевода (Google Translate), медицинской диагностики и т.д.
    - ResNet (разновидность Highway Net) позволил создавать сети с сотнями слоев.

---

#### **Критика и спорные моменты**
- Шмидхубер критикует "альтернативную историю" глубокого обучения, которая игнорирует ранние работы (например, Ивахненко и Амари) и приписывает ключевые идеи более поздним исследователям.
- Он подчеркивает важность правильного цитирования и отмечает случаи плагиата в научной литературе.

---

#### **Будущее ИИ**
- Автор прогнозирует экспоненциальный рост вычислительных мощностей и появление ИИ с уровнем интеллекта, превосходящим человеческий, к 2040 году ("Омега-точка").
- В долгосрочной перспективе ИИ может колонизировать космос, используя ресурсы Солнечной системы и за её пределами.

---

#### **Заключение**
Статья подчеркивает, что современный ИИ — это результат многовековых исследований, начиная с работ Лейбница и Гаусса. Шмидхубер призывает к объективному историческому анализу и признанию вклада всех ученых, заложивших основы этой области.