## Введение

На этом семинаре мы углубимся в работу со свёрточными сетями для классификации изображений.

### План семинара
1.  Обучение свёрточной сети и визуализация её фильтров.
2.  Изучение аугментаций данных и их роли.
3.  Знакомство с методами регуляризации и нормализации (Dropout, BatchNorm).
4.  Реализация Spatial Dropout.

## 1. Обучение свёрточной сети (VGG Baseline)

### Датасет CIFAR-10
*   **Описание**: Содержит 50000 тренировочных и 10000 тестовых изображений 10 классов (самолёты, автомобили, птицы, кошки, олени, собаки, лягушки, лошади, корабли, грузовики).
*   **Размер изображений**: 32x32 пикселя, 3 цветовых канала (RGB).
*   **Предобработка**: Нормализация с использованием средних значений и стандартных отклонений, специфичных для CIFAR-10.

```python
transform = transforms.Compose(
    [
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),
    ]
)
# Загрузка датасета и создание DataLoader'ов
cifar_train = torchvision.datasets.CIFAR10(root="./cifar", train=True, download=True, transform=transform)
cifar_val = torchvision.datasets.CIFAR10(root="./cifar", train=False, download=True, transform=transform)
train_dataloader = torch.utils.data.DataLoader(cifar_train, batch_size=32, shuffle=True, num_workers=4)
val_dataloader = torch.utils.data.DataLoader(cifar_val, batch_size=32, shuffle=False, num_workers=4)
```

### Архитектура VGG Baseline (Задание 1)
Базовая модель состоит из одного VGG блока и двух полносвязных слоёв:

1.  **VGG блок (16 каналов)**:
    *   `nn.Conv2d(3, 16, kernel_size=3, padding=1)` + `nn.ReLU()`
    *   `nn.Conv2d(16, 16, kernel_size=3, padding=1)` + `nn.ReLU()`
    *   `nn.MaxPool2d(kernel_size=2, stride=2)`
2.  **Полносвязные слои**:
    *   `nn.Linear(16 * 16 * 16, 128)` + `nn.ReLU()` (после Flatten)
    *   `nn.Linear(128, 10)` (для 10 классов CIFAR-10)

	```python
class ModelBaseline(nn.Module):
    def __init__(self):
        super(ModelBaseline, self).__init__()
        self.vgg = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(16 * 16 * 16, 128), # Входной размер после MaxPool2d (32/2=16), 16 каналов -> 16*16*16
            nn.ReLU(),
            nn.Linear(128, 10)
        )
    def forward(self, x):
        x = self.vgg(x)
        x = self.fc(x)
        return x
```

### Визуализация параметров сети (Фильтры)
*   `plot_filters(model_baseline.vgg[0], single_channel=False)`: Показывает фильтры первого свёрточного слоя как единые RGB изображения. Каждый фильтр "настроен" на поиск определённых цветовых или текстурных паттернов.
*   `plot_filters(model_baseline.vgg[0], single_channel=True)`: Показывает каждый канал (R, G, B) фильтра отдельно в оттенках серого. Более тёмные/светлые участки указывают на чувствительность фильтра к интенсивности соответствующего цветового канала в конкретной области.
*   `plot_filters(model_baseline.vgg[2], single_channel=True)`: Аналогично, но для фильтров второго свёрточного слоя. Они работают уже не с исходными RGB-данными, а с выходными фичами первого слоя, поэтому интерпретация становится сложнее.

### Эксперименты с функциями активации (Задание 2)
Замена `nn.ReLU` на другие функции активации (LeakyReLU, ELU, SELU) может влиять на:
*   **Качество**: Может улучшить или ухудшить точность.
*   **Скорость сходимости**: Некоторые активации могут ускорять или замедлять обучение.
*   **Проблема "умирающих" ReLU**: LeakyReLU, ELU, SELU помогают смягчить проблему, когда нейроны ReLU перестают активироваться.
*   **SELU**: Потенциально способствует само-нормализации, но может требовать определённой инициализации весов и специфичных условий.

Наблюдения из семинара: `nn.ELU` и `nn.SELU` показали лучшие результаты по точности, но `nn.SELU` значительно увеличил время обучения на эпохах.

```python
# Пример использования разных активаций
class ModelBaseline(nn.Module):
    def __init__(self, activation=nn.ReLU): # activation - принимаемый параметр
        super(ModelBaseline, self).__init__()
        self.vgg = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, padding=1),
            activation(), # Используем переданную функцию активации
            nn.Conv2d(16, 16, kernel_size=3, padding=1),
            activation(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(16 * 16 * 16, 128),
            activation(),
            nn.Linear(128, 10)
        )
    def forward(self, x):
        x = self.vgg(x)
        x = self.fc(x)
        return x

# Пример использования:
# model = ModelBaseline(activation=nn.LeakyReLU).to(device)
# model = ModelBaseline(activation=nn.ELU).to(device)
# model = ModelBaseline(activation=nn.SELU).to(device)
```

## 2. Аугментации

### Что такое аугментации?
*   **Цель**: Искусственное увеличение размера тренировочного датасета путём применения различных преобразований к существующим изображениям.
*   **Принцип**: Изменения достаточно малы, чтобы не менять смысловое содержание изображения (класс), но достаточно велики, чтобы модель видела больше вариаций данных.
*   **Польза**: Помогает снизить переобучение и улучшить обобщающую способность модели.
*   **Инструменты**: `torchvision.transforms.Compose` для объединения нескольких трансформаций.
*   **Примеры трансформаций**: `ColorJitter` (изменение яркости, контрастности, оттенка, насыщенности), `RandomHorizontalFlip` (случайное отражение по горизонтали), `RandomRotation` (случайный поворот).

```python
# Пример аугментаций:
transform = transforms.Compose(
    [
        transforms.ColorJitter(hue=0.05, saturation=0.05), # Изменение цвета
        transforms.RandomHorizontalFlip(),                  # Горизонтальное отражение
        transforms.RandomRotation(20),                      # Поворот на 20 градусов
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),
    ]
)
# Применяются только к тренировочной выборке. Валидационная выборка использует базовую нормализацию.
```

## 3. Регуляризация и нормализация в нейронных сетях

### Dropout
*   **Назначение**: Метод регуляризации для борьбы с переобучением, особенно в сложных сетях.
*   **Принцип**:
    *   **Во время тренировки (`model.train()`):** Случайным образом "выключает" (обнуляет) нейроны входного тензора с вероятностью `p`. Это заставляет сеть учиться более надёжным признакам, не полагаясь на какой-либо один нейрон.
    *   **Во время инференса (`model.eval()`):** Нейроны не обнуляются, но их значения масштабируются на `1/(1-p)`, чтобы сохранить ожидаемую сумму активаций, полученную во время обучения.
*   **PyTorch**: `torch.nn.Dropout(p)`.

### Batch Normalization (Пакетная нормализация)
*   **Назначение**: Стабилизация и ускорение сходимости обучения глубоких нейронных сетей.
*   **Принцип**: Нормализует выходы каждого слоя, приводя их к среднему 0 и дисперсии 1. Это снижает проблему внутренних ковариационных сдвигов.
*   **Размещение**: Обычно вставляется между свёрточными (или полносвязными) слоями и функцией активации.
*   **PyTorch**: `torch.nn.BatchNorm2d(num_features)` для свёрточных слоёв (где `num_features` — количество выходных каналов), `torch.nn.BatchNorm1d(num_features)` для полносвязных.
*   **Важно**:
    *   На тренировке: Среднее и дисперсия вычисляются по текущему мини-батчу.
    *   На инференсе: Используются скользящие средние и дисперсии, накопленные во время тренировки.

### Пример многоблочной VGG с BatchNorm и Dropout
```python
model = nn.Sequential(
    nn.Conv2d(3, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ReLU(),
    nn.Conv2d(16, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ReLU(),
    nn.MaxPool2d(2, 2),
    # nn.Dropout(0.2), # Можно добавить сюда обычный dropout

    nn.Conv2d(16, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),
    nn.Conv2d(32, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),
    nn.MaxPool2d(2, 2),
    # nn.Dropout(0.2),

    nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),
    nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),
    nn.MaxPool2d(2, 2),
    # nn.Dropout(0.2),

    nn.Flatten(),
    nn.Linear(64 * 4 * 4, 128), nn.BatchNorm1d(128), nn.ReLU(),
    nn.Dropout(0.5), # Обычный Dropout для полносвязных слоёв
    nn.Linear(128, 10),
)
```

### Задание 1. Реализация Spatial Dropout

*   **Проблема с обычным Dropout в CNN**: Обычный Dropout обнуляет отдельные элементы в тензорах фич, что не всегда оптимально для свёрточных слоёв. Поскольку соседние пиксели в карте признаков сильно коррелируют, обнуление отдельных пикселей может не дать желаемого эффекта регуляризации.
*   **Идея Spatial Dropout**: Обнуление целых каналов (целых карт признаков) вместо отдельных нейронов. Это заставляет модель учиться более разнообразным и независимым признакам в разных каналах.
*   **Форма входного тензора**: `[N, C, H, W]` (Batch, Channels, Height, Width).
*   **Механизм**: Для каждого изображения в батче случайным образом выбираются `C * p` каналов, которые обнуляются. Остальные каналы масштабируются на `1/(1-p)`.

```python
class SpatialDropout(nn.Module):
    def __init__(self, p=0.2):
        super(SpatialDropout, self).__init__()
        self.p = p

    def forward(self, x):
        if not self.training or self.p == 0.0:
            return x # В режиме eval() или p=0 возвращаем тензор как есть

        # x имеет форму [N, C, H, W]
        N, C, H, W = x.shape

        # Создаём маску для обнуления целых каналов
        # Генерируем случайный вектор длины C для каждого элемента батча
        # shape: [N, C, 1, 1] -> затем broadcast на H и W
        mask = (torch.rand(N, C, 1, 1, device=x.device, dtype=x.dtype) > self.p).float()

        # Масштабирование оставшихся каналов для сохранения ожидаемого значения
        return x * mask / (1.0 - self.p)

# Пример использования SpatialDropout в модели:
model = nn.Sequential(
    nn.Conv2d(3, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ReLU(),
    nn.Conv2d(16, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ReLU(),
    nn.MaxPool2d(2, 2),
    SpatialDropout(0.2),   # Spatial Dropout здесь

    nn.Conv2d(16, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),
    nn.Conv2d(32, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),
    nn.MaxPool2d(2, 2),
    SpatialDropout(0.3),

    nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),
    nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),
    nn.MaxPool2d(2, 2),
    SpatialDropout(0.4),

    nn.Flatten(),
    nn.Linear(64 * 4 * 4, 128), nn.BatchNorm1d(128), nn.ReLU(),
    nn.Dropout(0.5), # Обычный Dropout для полносвязных слоёв
    nn.Linear(128, 10),
)
```

---