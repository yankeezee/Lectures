

**1. Введение**
*   Лекция 1 посвящена введению в глубокое обучение.
*   Лектор: Евгений Соколов (esokolov@hse.ru).
*   Курс проводится в НИУ ВШЭ в 2025 году.

**2. Чем будем заниматься? (Мотивация)**
*   Лекция начинается с вопроса о задачах, которые будут рассматриваться.

**3. Пример задачи: Kaggle "Dogs vs. Cats"**
*   В качестве примера задачи машинного обучения приводится соревнование "Dogs vs. Cats" на Kaggle. Цель — создать алгоритм для классификации изображений на предмет наличия собаки или кошки. Указывается, что для людей, собак и кошек эта задача легка, но для компьютера — несколько сложнее.

**4. Классическое компьютерное зрение**
*   Традиционный подход в компьютерном зрении включает два основных шага:
    1.  **Извлечение признаков:** Вручную "считаются" признаки объекта (например, наличие усов, форма ушей, длина хвоста и т.д.).
    2.  **Обучение модели:** На этих извлеченных признаках обучается модель, часто с использованием градиентного бустинга.
*   Отмечается, что "посчитать признаки — целая история", подчеркивая сложность и трудоемкость ручного инжиниринга признаков.

**5. Современное компьютерное зрение**
*   Современный подход демонстрируется схемой сверточной нейронной сети (ResNet-подобная архитектура).
*   В отличие от классического подхода, признаки не извлекаются вручную, а "изучаются" самой сетью через последовательность слоев:
    *   `Conv1`: Первый сверточный слой с размером ядра 7x7 и 64 фильтрами.
    *   `MaxPool`: Слой пулинга.
    *   Последующие блоки `Conv2_x`, `Conv3_x`, `Conv4_x`, `Conv5_x` (предположительно, ResNet-идентити блоки) с различными размерами фильтров и количеством выходов (64, 128, 256, 512, 1024, 2048).
    *   `AvgPool`: Средний пулинг.
    *   `FC`: Полносвязный слой в конце.
*   Эта схема показывает, как глубокие нейронные сети автоматически извлекают и преобразуют признаки на разных уровнях абстракции.

**6. Классическое NLP (Обработка естественного языка)**
*   Классический подход в NLP:
    1.  **Статистический анализ:** Подсчитывается статистика, как часто одно слово встречается после другого.
    2.  **Генерация текста:** Следующее слово генерируется из этого статистического распределения (пример текста, сгенерированного на основе марковских цепей, иллюстрирует ограниченность такого подхода).
*   Приводится ссылка на статью "Making a text generator using markov chains" как пример.

**7. Современное NLP**
*   Современное NLP контрастирует с классическим, используя глубокие нейронные сети для более сложных задач, таких как генерация живого и интерактивного визуального опыта, реагирующего на музыку или взаимодействие пользователя. Показанный код (JavaScript/React) подразумевает способность к генерации текста и созданию сложных, отзывчивых систем.

**8. Успехи в глубинном обучении**
*   Глубокое обучение достигло значительных успехов в различных областях:
    *   Изображения и видео.
    *   Трехмерное компьютерное зрение.
    *   Тексты (включая код, математику).
    *   Звук и речь.
    *   Рекомендательные системы.
    *   Агентные системы.

**9. Организационное: О курсе**
*   **Вики:** http://wiki.cs.hse.ru/Основы_глубинного_обучения
*   **Канал Telegram:** @iad_2025
*   **Домашние задания:** Есть.
*   **Проверочные работы:** Неоцениваемые.
*   **Контрольная работа:** Есть.
*   **Письменный экзамен:** Есть.
*   **Автоматы:** Будет решено позже (возможно, для зачета без экзамена).

**10. Организационное: Про оценку**
*   Итоговая оценка рассчитывается по формуле: `Оитоговая = 0.4 * ДЗ + 0.3 * КР + 0.3 * Э`
    *   ДЗ: Домашние задания
    *   КР: Контрольная работа
    *   Э: Экзамен

**11. Примерный план курса**
*   Курс охватывает следующие темы:
    *   Метод обратного распространения ошибки (Backpropagation).
    *   Полносвязные сети.
    *   Сверточные сети.
    *   Методы оптимизации для глубинного обучения.
    *   Работа с последовательностями (Sequence processing).

**14. Предсказание стоимости квартиры: Линейная модель**
*   Пример линейной модели для предсказания стоимости квартиры: `a(x) = w0 + w1 * (площадь) + w2 * (этаж) + w3 * (расстояние до метро) + ...`
*   Отмечается ограничение: "Вряд ли признаки не связаны между собой", указывая на то, что линейные модели могут не учитывать сложные взаимодействия признаков.

**15. Предсказание стоимости квартиры: Линейная модель с полиномиальными признаками**
*   Для учета нелинейных взаимодействий можно добавить полиномиальные признаки: `a(x) = w0 + w1 * (площадь) + w2 * (этаж) + w3 * (расстояние до метро) + w4 * (площадь)^2 + w5 * (этаж)^2 + w6 * (расстояние до метро)^2 + w7 * (площадь) * (этаж) + ...`
*   Проблемы такого подхода:
    *   Может быть сложно интерпретировать модель.
    *   Возникает вопрос "Что такое (расстояние до метро) * (этаж)^2?", подчеркивающий сложность ручного инжиниринга и выбора осмысленных взаимодействий.

**16. Градиентный бустинг**
*   Краткий обзор градиентного бустинга:
    *   Модель представляется как сумма базовых моделей: `a_N(x) = Σ b_n(x)` (от n=1 до N).
    *   **Обучение N-й модели:** Включает минимизацию функции потерь `L` на текущих ошибках (`s_i^(N)`): `min(b_N(x)) (1/l) * Σ L(y_i, a_(N-1)(x_i) + b_N(x_i))` (от i=1 до l).
    *   Сдвиги (`s_i^(N)`) для обучения `b_N(x)` вычисляются как отрицательный градиент функции потерь по предсказанию предыдущей модели: `s_i^(N) = - (∂/∂z)L(y_i, z) |_(z=a_(N-1)(x_i))`.

**17. Кратко о предыдущем курсе (Machine Learning)**
*   **Линейные модели:** Обучаются градиентным спуском, но плохо подходят для поиска сложных закономерностей.
*   **Решающие деревья и их композиции:** Дают отличные результаты, но обучать их (в плане подбора сложных признаков) трудно.

**18. Нелинейные закономерности (Визуализация)**
*   Показывается пример данных с двумя классами (синие и оранжевые точки), которые не могут быть разделены одной прямой линией (нелинейно разделимы).
*   Демонстрируется, как две линейные границы могут быть использованы для разделения классов.
    *   Первая граница: `b1(x) = sign(-2x1 + x2 + 5)`
    *   Вторая граница: `b2(x) = sign(2x1 + x2 - 5)`
*   Комбинация этих двух простых линейных классификаторов позволяет выделить оранжевый класс условием: `Если b1(x) + b2(x) < 0, то оранжевый класс`.

**19. Нелинейные закономерности (Граф вычислений)**
*   Представленный выше подход переводится в граф вычислений:
    *   Входы `x1, x2`
    *   `z1 = -2x1 + x2 + 5`
    *   `z2 = 2x1 + x2 - 5`
    *   `b1 = sign(z1)`
    *   `b2 = sign(z2)`
    *   `a = b1 + b2`
    *   Выход: `[a < 0]` (для определения класса).

**20. Нейрон**
*   **Биологический нейрон:** Представлена схема биологического нейрона с его частями (ядро, тело клетки, аксон, дендриты, синаптические терминалы).
*   **Математическая модель нейрона:** Модель нейрона описывается как взвешенная сумма входов: `a(x) = Σ wjxj` (от j=1 до d).

**21. Нелинейные закономерности (Связь с нейронами/нейронными сетями)**
*   Граф вычислений из предыдущих слайдов аннотируется:
    *   Блоки `z1` и `z2`, а также их `sign` преобразования (`b1`, `b2`) обозначены как "как бы нейрон".
    *   Вся структура графа, объединяющая эти "нейроны", обозначена как "как бы нейронная сеть". Это устанавливает связь между решаемой задачей и архитектурой нейронных сетей.

**22. Граф вычислений (или нейронная сеть)**
*   Объясняется концепция графа вычислений как нейронной сети:
    *   `x(0)`: Признаки объекта (вход).
    *   `h1(x)`: Преобразование, называемое "слоем".
    *   `x(1)`: Результат (выход слоя).
*   **Последовательная сеть:** Демонстрируется простая последовательная структура: `x(0) → h1 → x(1) → h2 → x(2) → h3 → y`.
*   **Сеть с ветвлениями:** Показан более сложный граф с несколькими путями и выходами, иллюстрирующий, что нейронные сети могут быть сложными, нелинейными и иметь не только последовательную структуру.

**23. Полносвязные слои (Fully Connected, FC)**
*   **Определение:** Полносвязный слой принимает `n` чисел на входе и выдает `m` чисел на выходе.
    *   Входы: `x1, ..., xn`
    *   Выходы: `z1, ..., zm`
    *   Каждый выход `zj` является линейной моделью над входами: `zj = Σ (wjixi + bj)` (от i=1 до n).
*   **Схема:** Приводится классическая схема полносвязной нейронной сети с входным слоем, двумя скрытыми слоями и выходным слоем, где каждый нейрон одного слоя соединен со всеми нейронами следующего слоя.
*   **Параметры:**
    *   `m` линейных моделей, в каждой из которых `(n + 1)` параметров (n весов + 1 смещение).
    *   Всего примерно `mn` параметров в полносвязном слое.
    *   Пример: 1 000 000 входных признаков и 1000 выходов дают 1 000 000 000 параметров — очень много, требует большого объема данных для обучения.

**24. Важный вопрос в DL: Как объединить слои в мощную модель?**
*   Этот вопрос ведет к необходимости нелинейности.

**25. Нелинейность: Проблема последовательного соединения линейных слоев**
*   Рассматривается два последовательно соединенных полносвязных слоя.
*   Математически показано, что если `zj` являются линейной комбинацией `xi`, а `sk` являются линейной комбинацией `zj`, то `sk` в конечном итоге является просто линейной комбинацией `xi`.
*   **Вывод:** "То есть это ничем не лучше одного полносвязного слоя". Для создания мощных моделей необходимо добавить нелинейность.

**26. Нелинейность: Добавление нелинейной функции активации**
*   Для преодоления проблемы линейности необходимо добавлять нелинейную функцию `f` после каждого полносвязного слоя: `zj = f(Σ (wjixi + bj))` (от i=1 до n).

**27. Варианты нелинейных функций активации:**
*   **Вариант 1: Сигмоида (Sigmoid)**
    *   `f(x) = 1 / (1 + exp(-x))`
    *   График показывает S-образную кривую, выход находится в диапазоне (0, 1).
*   **Вариант 2: ReLU (REctified Linear Unit)**
    *   `f(x) = max(0, x)`
    *   График показывает, что для отрицательных входов выход равен 0, для положительных — равен входу. Сравнивается с Softplus.
*   **Другие функции активации:**
    *   Представлена таблица с формулами и графиками различных функций активации: Rectified linear unit (ReLU), Gaussian Error Linear Unit (GELU), Softplus, Exponential linear unit (ELU), Scaled exponential linear unit (SELU), Leaky rectified linear unit (Leaky ReLU), Parameteric rectified linear unit (PReLU), Sigmoid linear unit (SiLU) / Sigmoid shrinkage / Swish-1.
    *   График сравнивает формы нескольких активаций: SiLU, ReLU, Sigmoid, Swish, ELU.
*   **Продвинутые варианты GLU (Gated Linear Unit):**
    *   ReGLU, GEGLU, SwiGLU – варианты, используемые в архитектуре Transformer.
    *   Отмечается, что эти новые варианты улучшают перплексию для задачи де-шумления при предварительном обучении и дают лучшие результаты во многих задачах понимания языка. Они просты в реализации и не имеют явных вычислительных недостатков.

**28. Типичная полносвязная сеть**
*   Иллюстрируется типичная архитектура полносвязной сети: `x(0) → FC1 → f → FC2 → f → ...`
*   **Вход:** На вход подаются признаки.
*   **Выход:** В последнем слое количество выходов равно количеству целевых переменных, которые мы предсказываем.

**29. Теорема Цыбенко (Теорема универсальной аппроксимации)**
*   **Вольное изложение:**
    *   Пусть `g(x)` — непрерывная функция.
    *   Тогда можно построить двуслойную нейронную сеть, приближающую `g(x)` с любой заранее заданной точностью.
*   **Вывод:** "То есть двуслойные нейронные сети ОЧЕНЬ мощные!"
*   **Ограничение:** "Но очень много параметров и очень сложно обучать".

**30. Резюме**
*   **Идея глубинного обучения:** Совмещение большого количества дифференцируемых слоев.
*   **Извлечение признаков:** Слои извлекают сложные признаки из данных.
*   **Полносвязные слои:** Самый простой (и при этом мощный) вариант.
*   **Нелинейности:** Крайне важны для построения мощных моделей.

---