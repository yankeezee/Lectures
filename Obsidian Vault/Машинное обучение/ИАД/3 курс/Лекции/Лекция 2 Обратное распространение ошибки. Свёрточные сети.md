
### Обучение нейронных сетей

#### Опрос: Формула для шага в градиентном спуске
Какая из следующих формул представляет шаг в градиентном спуске?
1.  $w^t = w^{t-1} + \eta \nabla Q(w^t)$
2.  $w^t = w^{t-1} - \eta \nabla Q(w^{t-1})$
3.  $w^t = w^{t-1} - \eta \nabla Q(w^t)$
4.  $w^t = w^{t-1} + \eta \nabla Q(w^0)$

**Правильный ответ:** Формула для шага в градиентном спуске - это $w^t = w^{t-1} - \eta \nabla Q(w^{t-1})$.

#### Градиентный спуск
*   **Идея:** Повторять до сходимости.
*   **Формула шага:** $w^t = w^{t-1} - \eta \nabla Q(w^{t-1})$
    *   $w^t$: Новая точка (обновленные веса на шаге $t$).
    *   $w^{t-1}$: Веса на предыдущем шаге.
    *   $\eta$: Размер шага (скорость обучения).
    *   $\nabla Q(w^{t-1})$: Градиент функции потерь $Q$ в предыдущей точке $w^{t-1}$.

#### Сходимость
Процесс обучения останавливается, если:
*   Разница между текущими и предыдущими весами становится очень малой: $||w^t - w^{t-1}|| < \epsilon$.
*   Или, если норма градиента становится очень малой: $||\nabla Q(w^t) || < \epsilon$.
*   **В глубоком обучении:** Обычно останавливаются, когда ошибка на тестовой выборке перестает убывать.

#### Обучение нейронных сетей (продолжение)
*   Все слои в нейронной сети обычно дифференцируемы, что позволяет вычислять производные по всем параметрам.
*   **Пример сети:** Вход $x \rightarrow FC_1 \rightarrow f \rightarrow FC_2 \rightarrow a(x)$. На выходе сравнивается $a(x)$ с истинным значением $y$ с помощью функции потерь $L(y, a(x))$.
*   **Функция активации:** $a(x) = FC_2(f(FC_1(x)))$.
*   **Параметры:** Параметры находятся внутри полносвязных слоев $FC_1$ и $FC_2$ (веса и смещения).
*   **Цель обучения:** Минимизировать среднюю ошибку по обучающей выборке: $\frac{1}{\ell} \sum_{i=1}^{\ell} L(y_i, a(x_i)) \rightarrow \min_a$.

#### Считаем производные
*   Для градиентного спуска необходимы производные ошибки по параметрам: $\frac{\partial}{\partial w_j} L(y_i, a(x_i, w))$.
*   **Пример с квадратичной ошибкой:** Если функция потерь - это квадрат разности $(a(x_i, w) - y_i)^2$, то ее производная по параметру $w_j$ будет:
    $\frac{\partial}{\partial w_j} (a(x_i, w) - y_i)^2 = 2(a(x_i, w) - y_i) \frac{\partial}{\partial w_j} a(x_i, w)$.
    *   $2(a(x_i, w) - y_i)$: показывает, насколько сильно изменится ошибка, если пошевелить $a(x_i, w)$.
    *   $\frac{\partial}{\partial w_j} a(x_i, w)$: показывает, насколько сильно изменится $a(x_i, w)$, если пошевелить $w_j$.
*   **Примеры значений:**
    *   Если $a(x_i, w) = 10$ и $y_i = 9.99$: $2 \times 0.01 \times \frac{\partial}{\partial w_j} a(x_i, w)$.
    *   Если $a(x_i, w) = 10$ и $y_i = 1$: $2 \times 9 \times \frac{\partial}{\partial w_j} a(x_i, w)$.
*   **Общий случай (цепное правило):**
    $\frac{\partial}{\partial w_j} L(y_i, a(x_i, w)) = \frac{\partial}{\partial z} L(y_i, z) \Big|_{z=a(x_i, w)} \frac{\partial}{\partial w_j} a(x_i, w)$.
    *   $\frac{\partial}{\partial z} L(y_i, z) \Big|_{z=a(x_i, w)}$: показывает, насколько сильно изменится ошибка, если пошевелить $a(x_i, w)$.
    *   $\frac{\partial}{\partial w_j} a(x_i, w)$: показывает, насколько сильно изменится $a(x_i, w)$, если пошевелить $w_j$.
*   **Следующая задача:** Научиться вычислять $\frac{\partial}{\partial w_j} a(x_i, w)$.

#### Как считать производные? (Обратное распространение ошибки)
*   **Сеть:** Пример сети с двумя входными нейронами ($x_1, x_2$), двумя скрытыми слоями ($z_1, z_2$ и $h_1, h_2$) и одним выходным нейроном ($a$).
*   **Прямой проход (пример):**
    *   Вход: $x_1=5, x_2=-1$.
    *   Промежуточные значения: $z_1 = 7, z_2 = -1$.
    *   Промежуточные значения: $h_1 = 0, h_2 = 13$.
    *   Выход: $a = -13$.
*   **Вычисление производных с помощью цепного правила (обратный проход):**
    *   **Пример:** $a(x) = p_{11}h_1(x) + p_{21}h_2(x)$.
    *   $\frac{\partial a}{\partial p_{11}} = h_1(x)$. (Чем больше $h_1(x)$, тем сильнее $p_{11}$ влияет на $a$).
    *   **Для более глубоких слоев:** $\frac{\partial a}{\partial v_{11}} = \frac{\partial a}{\partial h_1} \frac{\partial h_1}{\partial v_{11}}$.
    *   **Пример зависимости:** $\frac{\partial a}{\partial w_{11}}$ показывает, как сильно изменится $a$ при изменении $w_{11}$.
        *   $w_{11}$ влияет на $z_1$, который влияет на $h_1$ и $h_2$, которые влияют на $a$.
        *   $v_{11}$ влияет на $h_1$, который влияет на $a$.
        *   $v_{12}$ влияет на $h_2$, который влияет на $a$.
        *   $p_{11}$ напрямую влияет на $a$ через $h_1$.
        *   $w_{22}$ влияет на $z_2$, который влияет на $h_1$ и $h_2$, которые влияют на $a$.
        *   $v_{22}$ влияет на $h_2$, который влияет на $a$.
    *   **Полное цепное правило для $w_{11}$:**
        $\frac{\partial a}{\partial w_{11}} = \frac{\partial a}{\partial h_1} \frac{\partial h_1}{\partial z_1} \frac{\partial z_1}{\partial w_{11}} + \frac{\partial a}{\partial h_2} \frac{\partial h_2}{\partial z_1} \frac{\partial z_1}{\partial w_{11}}$.
*   Мы как бы идем в обратную сторону по графу и считаем производные.
*   Этот метод называется **методом обратного распространения ошибки (backpropagation)**.

#### Backprop (продолжение)
*   Многие формулы содержат одни и те же производные.
*   В backprop каждая частная производная вычисляется один раз.
*   Вычисление производных по слою $N$ сводится к перемножению матрицы производных по слою $N+1$ и некоторых векторов.
*   Это позволяет эффективно вычислять все необходимые градиенты.

---

### Полносвязные сети для изображений

#### MNIST
*   **Пример:** MNIST датасет состоит из рукописных цифр.
*   **Характеристики:**
    *   Изображения 28 x 28 пикселей.
    *   Изображения центрированы.
    *   60 000 объектов в обучающей выборке.
*   **Что может выучить полносвязная сеть?**
    *   Примерно 784 входа (28x28 пикселей).
    *   Сеть обучается детектировать заполненность конкретного набора пикселей.
*   **Ограничения полносвязных сетей для изображений:**
    *   Если немного сдвинуть цифру, нейрон уже не будет на нее реагировать, так как он "привязан" к конкретным пикселям. Сети не инвариантны к сдвигам.

#### Число параметров в полносвязных сетях
*   **Пример сети для MNIST:**
    *   784 входа.
    *   Полносвязный скрытый слой: 1000 нейронов.
    *   Выходной слой: 10 нейронов (по одному на каждый класс).
*   **Расчет параметров:**
    *   Весов между входным и полносвязным слоями: $(784 + 1) \times 1000 = 785 000$ (включая смещения).
    *   Весов между полносвязным и выходным слоями: $(1000 + 1) \times 10 = 10 010$ (включая смещения).
*   **Итого:** Общее количество параметров очень велико, что приводит к проблемам.
*   **Качество:** Можно добиться хорошего качества полносвязными сетями (с аугментацией данных).
    *   Таблица ошибок на MNIST показывает, что даже с глубокими полносвязными сетями (например, 2500, 2000, 1500, 1000, 500, 10 нейронов) достигается ошибка 0.35% на валидации при более чем 12 миллионах параметров.

#### Проблемы полносвязных слоев для изображений
*   Очень много параметров.
*   Легко могут переобучиться.
*   Не учитывают специфику изображений (сдвиги, небольшие изменения формы и т.д.).
*   Один из лучших способов борьбы с переобучением — снижение числа параметров.

---

### Свёртки (Convolutions)

#### Эксперименты со зрительной корой
*   Исследования зрительной коры показали, что нейроны в ней реагируют на определенные ориентации линий и краев, независимо от их точного положения в поле зрения. Это дало мотивацию для создания сверточных сетей.

#### Что такое свёртка?
*   **Операция:** Свёртка выявляет наличие паттерна на изображении, который задается фильтром.
*   **Принцип:** Чем сильнее на участке изображения представлен паттерн, тем больше будет значение свёртки.
*   **Как работает:**
    1.  Фильтр (ядро) сдвигается по входному изображению.
    2.  На каждой позиции выполняется поэлементное умножение значений фильтра на соответствующие пиксели изображения.
    3.  Результаты поэлементного умножения суммируются, давая одно значение в выходном (свернутом) изображении.
*   **Пример:** Свертка с фильтром $ \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix} $ на фрагменте изображения $ \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} $ дает результат $1 \times 1 + 0 \times 2 + 0 \times 3 + 1 \times 4 = 1+0+0+4=5$.

#### Максимум свёртки инвариантен к сдвигам
*   Если паттерн на изображении немного сдвигается, максимальное значение, полученное после свертки (или после операции макс-пулинга, которая часто следует за сверткой), остается тем же. Это свойство называется **инвариантностью к сдвигам**.
*   Пример: Паттерн '1' сдвигается по входному изображению. Максимум в выходном изображении (после свертки с определенным фильтром) остается равным 2, независимо от точного положения '1'.

#### Свёртки в компьютерном зрении (Примеры фильтров)
*   **Горизонтальный фильтр Собеля:** $ \begin{pmatrix} +1 & 0 & -1 \\ +2 & 0 & -2 \\ +1 & 0 & -1 \end{pmatrix} $ используется для выделения горизонтальных краев.
*   **Фильтр повышения резкости (Sharpening):** $ \frac{1}{9} \begin{pmatrix} -1 & -1 & -1 \\ -1 & 9 & -1 \\ -1 & -1 & -1 \end{pmatrix} $.
*   **Фильтр размытия (Blurring):** $ \frac{1}{9} \begin{pmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{pmatrix} $.

#### Математическая формула свёртки
*   **2D свертка:** Выходной пиксель $(x, y)$ в свернутом изображении $Im^{out}$ вычисляется как:
    $Im^{out}(x, y) = \sum_{i=-d}^{d} \sum_{j=-d}^{d} (K(i,j) Im^{in}(x + i, y + j) + b)$.
    *   $K(i, j)$ - значение фильтра (ядра) в позиции $(i, j)$.
    *   $Im^{in}(x+i, y+j)$ - значение входного пикселя.
    *   $b$ - смещение (bias).
*   **Ключевые свойства из формулы:**
    *   **Local connectivity (локальная связность):** Пиксель в результирующем изображении зависит только от небольшого участка исходного изображения.
    *   **Shared weights (разделяемые веса):** Веса (значения фильтра $K$) одни и те же для всех пикселей результирующего изображения.

#### Свёртка для цветных изображений (многоканальная)
*   Обычно исходное изображение цветное (например, R, G, B каналы).
*   Формула учитывает несколько каналов (C):
    $Im^{out}(x, y) = \sum_{i=-d}^{d} \sum_{j=-d}^{d} \sum_{c=1}^{C} (K(i, j, c) Im^{in}(x + i, y + j, c) + b)$.
    *   $K(i, j, c)$ - фильтр имеет глубину, соответствующую количеству входных каналов.
*   **Визуализация:** Входное изображение имеет HxWxCin (высота, ширина, количество входных каналов). Фильтр имеет Wk x Hk x Cin (размерность ядра). Результат - одно выходное значение для одного фильтра.

#### Создание многоканального выходного изображения (несколько фильтров)
*   Одна свёртка выделяет конкретный паттерн.
*   Нам интересно искать много паттернов.
*   Для этого используется несколько фильтров, каждый из которых создает свой выходной канал.
*   Результат становится трехмерным (HxWxT, где T - количество фильтров/выходных каналов):
    $Im^{out}(x, y, t) = \sum_{i=-d}^{d} \sum_{j=-d}^{d} \sum_{c=1}^{C} (K_t(i, j, c) Im^{in}(x + i, y + j, c) + b_t)$.
    *   $K_t$ и $b_t$ - параметры $t$-го фильтра.

#### Число параметров в свёрточных слоях
*   В сверточном слое обучается только фильтр (или набор фильтров).
*   Количество параметров для слоя с $T$ фильтрами: $((2d + 1)^2 \times C + 1) \times T$.
    *   $(2d+1)^2$: размер фильтра (например, для $d=1$, фильтр $3 \times 3$).
    *   $C$: количество входных каналов.
    *   $+1$: для смещения (bias) для каждого фильтра.
    *   $T$: количество фильтров (количество выходных каналов).
*   Это значительно меньше параметров, чем в полносвязных сетях, что помогает бороться с переобучением и делает сети инвариантными к сдвигам.

---