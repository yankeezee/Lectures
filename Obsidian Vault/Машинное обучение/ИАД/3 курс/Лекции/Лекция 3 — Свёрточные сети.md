
### 1. Свёртка (Convolution)

Свёртка — это базовая операция в свёрточных нейронных сетях. Она включает в себя три основных элемента:
*   **Вход (Input):** Исходные данные, например, изображение.
*   **Фильтр (Filter):** Небольшая матрица весов, которая скользит по входным данным.
*   **Выход (Output):** Результат применения фильтра ко входным данным.

Процесс свёртки состоит из поэлементного умножения элементов фильтра на соответствующий участок входных данных, а затем суммирования этих произведений для получения одного элемента выходных данных.

Например, для входной матрицы и фильтра, как показано на слайде, фильтр применяется к участку входных данных. Значение 7 в выходной матрице получается в результате поэлементного умножения выбранного участка входной матрицы на фильтр, а затем суммирования результатов.

### 2. Свёртка инвариантна к сдвигам (Convolution is Shift-Invariant)

Одним из ключевых свойств свёртки является её инвариантность к сдвигам. Это означает, что если паттерн во входном изображении сдвигается, то и соответствующий отклик в выходном изображении также сдвигается, но сам паттерн детектируется тем же фильтром.

На слайде показано, что сдвиг активного пикселя во входной матрице приводит к сдвигу максимального значения в выходной матрице, но само значение Max=2 остаётся неизменным. Это свойство делает свёрточные сети эффективными для обнаружения признаков независимо от их положения в изображении.

### 3. Свёртки в компьютерном зрении (Convolutions in Computer Vision)

Свёртки широко используются в компьютерном зрении для извлечения различных признаков. Например, для обнаружения горизонтальных рёбер может быть использовано ядро Собеля:
$\begin{pmatrix} +1 & 0 & -1 \\ +2 & 0 & -2 \\ +1 & 0 & -1 \end{pmatrix}$
Применение такого ядра к изображению позволяет выделить горизонтальные грани.

### 4. Математическая Формула Свёртки (Convolution Formula)

Базовая формула свёртки для одномерного входного изображения$Im^{in}$и выходного изображения$Im^{out}$с использованием фильтра$K$и смещения$b$(bias):
$Im^{out}(x, y) = \sum_{i=-d}^{d} \sum_{j=-d}^{d} (K(i,j) Im^{in}(x + i, y + j) + b)$
Здесь$d$определяет размер фильтра (например, для фильтра$3 \times 3$,$d=1$).

**Свойства:**
*   **Локальная связность (local connectivity):** Пиксель в результирующем изображении зависит только от небольшого участка исходного изображения.
*   **Общие веса (shared weights):** Веса фильтра ($K(i,j)$) одни и те же для всех пикселей результирующего изображения.

### 5. Свёртка с каналами (Convolution with Channels)

Обычно исходное изображение цветное, что означает наличие нескольких каналов (например, R, G, B). Для учёта этого, формула свёртки расширяется:
$Im^{out}(x, y) = \sum_{i=-d}^{d} \sum_{j=-d}^{d} \sum_{c=1}^{C} (K(i, j, c) Im^{in}(x + i, y + j, c) + b)$
Здесь$C$— количество входных каналов. Фильтр теперь имеет размерность$W_k \times H_k \times C_{in}$.

### 6. Несколько паттернов (Multiple Patterns)

Одна свёртка выделяет конкретный паттерн. Для поиска множества паттернов результат делается трёхмерным, т.е. сеть производит несколько выходных карт признаков (feature maps).
$Im^{out}(x, y, t) = \sum_{i=-d}^{d} \sum_{j=-d}^{d} \sum_{c=1}^{C} (K_t(i, j, c) Im^{in}(x + i, y + j, c) + b_t)$
Здесь$t$индексирует выходные паттерны.

### 7. Число параметров (Number of Parameters)

В свёрточном слое обучается только фильтр (ядро свёртки) и смещение (bias). Число параметров вычисляется следующим образом:
$((2d + 1)^2 \cdot C + 1) \cdot T$
где$2d+1$- размерность фильтра (например, 3 для$d=1$),$C$- число входных каналов,$+1$- для смещения (bias),$T$- число выходных паттернов (фильтров).

### 8. Поле восприятия (Receptive Field)

Поле восприятия (receptive field) — это область входного изображения, которая влияет на один пиксель в итоговом выходном изображении после нескольких свёрточных слоёв.

**Как растёт поле восприятия для свёртки$3 \times 3$:**
*   После 1 свёрточного слоя:$3 \times 3$
*   После 2 свёрточных слоёв:$5 \times 5$
*   После 3 свёрточных слоёв:$7 \times 7$

**Как растёт поле восприятия для свёртки$5 \times 5$:**
*   После 1 свёрточного слоя:$5 \times 5$
*   После 2 свёрточных слоёв:$9 \times 9$
*   После 3 свёрточных слоёв:$13 \times 13$

Если изображение имеет размер, например,$512 \times 512$, требуется очень много слоёв, чтобы поле восприятия последних слоёв охватывало всю картинку.

### 9. Свёртки с пропусками (Strided Convolutions)

Свёртки с пропусками (strides) позволяют уменьшить размер выходного изображения и увеличить поле восприятия последующих слоёв. Если stride$s=2$, фильтр сдвигается на 2 пикселя за раз.

Например, для фильтра$3 \times 3$со stride$s=2$, поле восприятия может быть$7 \times 7$после определённого количества операций.

Подробности о расчёте поля восприятия можно найти на `https://distill.pub/2019/computing-receptive-fields/`

### 10. Разреженные свёртки (Dilated Convolutions)

Разреженные (или "раздутые") свёртки (dilated convolutions) позволяют увеличить поле восприятия без увеличения числа параметров или уменьшения разрешения. Они вводят "дырки" в фильтр, пропуская пиксели.

*  $l=1$: Обычная свёртка.
*  $l=2$: Фильтр пропускает один пиксель между элементами.
*  $l=4$: Фильтр пропускает три пикселя между элементами.

Это позволяет эффективно захватывать информацию из более широкого контекста.

### 11. Пулинг (Pooling)

Пулинг (pooling) — это операция, которая уменьшает пространственный размер представления, сокращает количество параметров и вычислений в сети.
*   Разбивает изображение на участки$n \times m$.
*   Считает некоторую статистику в каждом участке (обычно максимум - Max-pooling).
*   Существенно сокращает размер изображения, что увеличивает поле восприятия следующих слоёв.
*   Не имеет параметров.

Пример Max-pooling с фильтром$2 \times 2$: из каждого участка$2 \times 2$выбирается максимальное значение.

**Зачем это всё?** Важно следить за тем, чтобы последние свёрточные слои имели размер поля восприятия, сравнимый со всей картинкой.

### 12. Паддинг (Padding)

Паддинг (padding) — это добавление дополнительных пикселей (обычно нулей) по границам входного изображения.

**Проблема без паддинга:** Если применять свёртку по формуле без паддинга, выходное изображение будет меньше входного.

**Valid mode:** При честном подсчёте свёрток пиксели на краях оказывают меньшее влияние на результат, и фильтр может не "увидеть" хороший отклик, если его центр находится близко к краю.

**Типы паддинга:**
*   **Zero padding:** Добавляет нули по границам, чтобы свёртка в valid mode давала изображение такого же размера, как исходное. Риск: модель может научиться понимать, где на изображении края, что может привести к потере инвариантности.
*   **Reflection padding:** Отражает пиксели от границ. Не получится легко находить края изображения. Модель может начать находить зеркальные отражения и подбирать фильтры под них.
*   **Replication padding:** Повторяет граничные пиксели. Пиксель на границе равен ближайшему пикселю из изображения. Модель всё ещё может настроиться под паттерны, которые возникают из-за такого паддинга.

**Резюме по паддингу:**
*   Позволяет контролировать размер выходных изображений.
*   Позволяет учитывать объекты на краях.
*   Разные типы паддингов допускают разные способы переобучения под края.

### 13. Структура свёрточных сетей (Convolutional Network Structure)

Типичная архитектура свёрточной нейронной сети включает:
*   Последовательное применение комбинаций вида:
    *   «свёрточный слой -> нелинейность -> pooling»
    *   или «свёрточный слой -> нелинейность»
*   Выпрямление (flattening) выхода очередного слоя.
*   Серия полносвязных слоёв.

**Примеры архитектур:**
*   **LeNet:** Одна из первых успешных свёрточных сетей, разработанная Яном ЛеКуном, используется для распознавания рукописных цифр. `http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf`
*   **AlexNet:** Прорывная архитектура, показавшая высокую производительность на ImageNet, включает несколько свёрточных и пулинг слоёв, за которыми следуют полносвязные слои. `http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf`

### 14. Представления с последних слоёв (Representations from Last Layers)

Важное наблюдение: выходы полносвязных слоёв являются хорошими признаковыми описаниями изображений.
*   Полезны во многих задачах, например, поиск похожих изображений.
*   По смыслу являются «индикаторами» наличия каких-то паттернов, но не интерпретируются напрямую (в отличие от классического компьютерного зрения).

Визуализации показывают, что ранние слои (Layer 1) обнаруживают простые признаки, такие как рёбра и цветовые переходы, тогда как более глубокие слои (Layer 2, 3, 4, 5) учатся распознавать всё более сложные и абстрактные паттерны, вплоть до частей объектов и целых объектов.

### 15. Стохастический градиентный спуск (Stochastic Gradient Descent)

**Градиентный спуск (GD):**
1.  Начальное приближение:$w^0$
2.  Повторять:$w^t = w^{t-1} - \eta \nabla Q(w^{t-1})$
3.  Останавливаемся, если ошибка на тестовой выборке перестаёт убывать.

**Стохастический градиентный спуск (SGD):**
1.  Начальное приближение:$w^0$
2.  Повторять, каждый раз выбирая случайный объект$i_t$:$w^t = w^{t-1} - \eta \nabla L(y_{i_t}, a(x_{i_t}))$
3.  Останавливаемся, если ошибка на тестовой выборке перестаёт убывать.

*   Оценка по одному объекту является несмещённой, то есть в среднем мы идём в правильную сторону.
*   Даже в точке оптимума оценка по одному объекту вряд ли будет нулевой.
*   Поэтому важно, чтобы длина шага$\eta_t$стремилась к нулю, например,$\eta_t = \frac{0.1}{t^{0.3}}$.
*   Сходимость к глобальному минимуму гарантируется только для выпуклых функций.

**Mini-batch GD:**
1.  Начальное приближение:$w^0$
2.  Повторять, каждый раз выбирая$m$случайных объектов$i_1, ..., i_m$:
   $w^t = w^{t-1} - \eta_t \frac{1}{n} \sum_{j=1}^{n} \nabla L(y_{t,j}, a(x_{t,j}))$
    Здесь$x_{t,j}$— объект номер$j$из батча, сформированного на шаге$t$.
3.  Останавливаемся, если ошибка на тестовой выборке перестаёт убывать.

**Размер батча (Batch size):**
*   Обычно порядка десятков или сотен.
*   Имеет смысл брать степень двойки.
*   Может делать оценку градиента более стабильной.
*   Вычислительно почти так же эффективен, как шаг по градиенту одного объекта, за счёт векторизации.

Исследования показывают, что увеличение размера мини-батча прогрессивно снижает диапазон скоростей обучения для стабильной сходимости. Лучшая производительность достигалась для размеров батчей от$m=2$до$m=32$.
Ян ЛеКун высказывает мнение, что использование больших мини-батчей (более 32) обусловлено неэффективностью GPU для меньших размеров и приводит к худшей ошибке на тестовых данных, а очень большие батчи (более 1024) "учат быстро, но неправильно".

Важно грамотно подбирать формулу для длины шага и/или постепенно увеличивать размер батча.

### 16. Модификации градиентного спуска (Modifications of Gradient Descent)

**Проблемы GD:**
*   Если у функции «вытянуты» линии уровня, то градиентный спуск требует аккуратного выбора длины шага и будет долго сходиться, осциллируя.

**Momentum (Метод моментов):**
Вводит «инерцию» — усреднённое направление движения.
$h_t = \alpha h_{t-1} + \eta \nabla Q(w^{t-1})$
$w^t = w^{t-1} - h_t$
Где$h_t$— это "инерция", а$\alpha$— параметр затухания. Это как будто шарик, который катится в сторону минимума, очень тяжёлый, что помогает преодолевать мелкие локальные минимумы и двигаться более стабильно в правильном направлении.

**Nesterov Momentum (Ускоренный градиент Нестерова):**
Модификация Momentum, которая "заглядывает вперёд" перед вычислением градиента.
$h_t = \alpha h_{t-1} + \eta_t \nabla Q(w^{t-1} - \alpha h_{t-1})$
$w^t = w^{t-1} - h_t$
Здесь$w^{t-1} - \alpha h_{t-1}$— это неплохая оценка того, куда мы попадём на следующем шаге.

---