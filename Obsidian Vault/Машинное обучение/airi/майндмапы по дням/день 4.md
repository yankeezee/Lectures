### Лекция «Фундаментальные и практические вопросы защиты данных в эпоху Генеративного ИИ»

#### **1. Введение и общая картина**
- **Ключевые концепции**:
  - $\text{Приватность данных} \in \text{Критические параметры}$
  - $\text{Безопасность ИИ} = f(\text{генеративные модели})$
- **Свойства систем**:
  - Конфиденциальность (по ФСТЭК №117)
  - Масштабируемость: $\lim_{n\to\infty} \text{Perf}(n) > C$

#### **2. Экономика утечек**
- **Функция затрат**:
  $$\text{Ущерб}_{2024} = \sum_{i=1}^{n} (C_i^{прям.} + C_i^{косв.}) \approx 10^8 \text{ руб.}$$
- **Оптимизация**:
  $$\min_{\theta} \left[ \alpha \cdot \text{ИИ-методы} + (1-\alpha)\cdot\text{Трад.методы} \right]$$

#### **3. Конвейер защиты**
1. **Сбор данных**: 
   - $\mathcal{D} = \{x_i,y_i\}_{i=1}^n \rightarrow \text{Анонимизация}$
2. **Обучение**:
   - $\arg\min_w \mathcal{L}(w; \mathcal{D}_{secure})$
3. **Инференс**:
   - $\text{SecureAPI}(x) = \sigma(Wx + b)$

#### **4. Глобальная регуляторика**
- **Матрица регулирования**:
  $$\begin{bmatrix}
  \text{ЕС} & \text{Жёсткое} & 46.5M€ \\
  \text{США} & \text{Добровольное} & 10M\$ \\
  \end{bmatrix}$$

#### **5. Уязвимости GenAI**
- **Атаки**:
  - Membership Inference: $P(x \in \mathcal{D}_{train}) > \tau$
  - Бэкдоры: $\exists x' \text{ s.t. } f(x') \neq f_{clean}(x')$
  
#### **6. Кейсы защиты**
- **Водяные знаки**:
  $$\text{Model}_{\text{secure}} = \text{Model} \oplus \text{WM}(k)$$
- **UAP-патчи**:
  $$\min_\delta \|\delta\| \text{ s.t. } \text{ASR}(\delta) \geq 95\%$$

#### **7. Machine Unlearning**
- **Формализация**:
  $$\text{CLEAR}: \frac{\partial \mathcal{L}}{\partial w} \rightarrow \text{Forget}(x')$$
- **Методы**:
  - SCRUB: $\mathcal{L}_{unlearn} = \mathcal{L}_{orig} - \lambda \mathcal{L}_{forget}$

#### **8. Финансовые риски**
- **Модель волатильности**:
  $$\sigma_{AI} = \beta \cdot \sigma_{market} + (1-\beta)\cdot \epsilon_{HFT}$$

#### **Заключение**
- **Принцип Тьюринга**:
  $$\lim_{t\to\infty} \frac{\text{Безопасность}}{\text{Развитие ИИ}} = \text{const}$$
### Лекция «Теоретические основы распределенной и асинхронной оптимизации»

#### **1. Введение и мотивация**
- **Федеративное обучение**:
  - Формализация задачи:
    $$\min_{x \in \mathbb{R}^d} f(x) = \frac{1}{n} \sum_{i=1}^n f_i(x)$$
    где $f_i(x) = \mathbb{E}_{\xi \sim D_i}[f_i(x;\xi)]$
  - Проблемы:
    - Гетерогенность устройств: $\tau_i \neq \tau_j$
    - Разные распределения данных: $D_i \neq D_j$

#### **2. Постановка задачи**
- **Невыпуклая оптимизация**:
  - Цель: найти стационарную точку
    $$\mathbb{E}[\|\nabla f(x)\|^2] \leq \varepsilon$$
- **Стохастические градиенты**:
  - Условия:
    $$\mathbb{E}_\xi[\nabla f(x;\xi)] = \nabla f(x)$$
    $$\mathbb{E}_\xi[\|\nabla f(x;\xi) - \nabla f(x)\|^2] \leq \sigma^2$$

#### **3. Основные предположения**
- **L-гладкость**:
  $$\|\nabla f(x) - \nabla f(y)\| \leq L\|x - y\|$$
- **Ограниченная дисперсия**:
  $$\sigma^2 = \sup_x \mathbb{E}[\|\nabla f(x;\xi) - \nabla f(x)\|^2]$$

#### **4. Методы и сложность**
| Метод          | Обновление параметров | Сложность |
|----------------|-----------------------|-----------|
| Minibatch SGD  | $x^{k+1} = x^k - \frac{\gamma}{n}\sum_{i=1}^n \nabla f_i(x^k)$ | $O(\frac{L\Delta}{\epsilon} + \frac{\sigma^2}{n\epsilon^2})$ |
| ASGD           | $x^{k+1} = x^k - \gamma \nabla f_{i_k}(x^{k-\tau_k})$ | Адаптивная |
| Rennala SGD    | $\frac{1}{S}\sum_{j=1}^S \nabla f_{i_j}(x^r)$ | Оптимальная |

#### **5. Нижние границы**
- **Фундаментальный предел**:
     $\Omega \left( \min_{m \in [n]} \left[ \left( \frac{1}{m} \sum_{i=1}^m \frac{1}{\tau_i} \right)^{-1} \left( \frac{L \Delta}{\epsilon} + \frac{\sigma^2 L \Delta}{m \epsilon^2} \right) \right] \right).$

#### **6. Универсальная модель**
- **Динамические скорости**:
  $$V_i(t) = \int_0^t v_i(\tau)d\tau$$
- **Гарантии**:
  - Rennala/Malenia сохраняют оптимальность

#### **7. Применения**
1. **Рекомендательные системы**:
   - Асинхронные обновления: $\delta_k \leq 1\text{час}$
2. **Медицинские изображения**:
   - Гетерогенные данные: $f_i(x) \neq f_j(x)$
3. **NLP-модели**:
   - Сжатие градиентов: $\|\mathcal{C}(g) - g\| \leq \delta$

#### **8. Ключевые методы**
- **Оптимальные алгоритмы**:

### Лекция «Интерпретируемость больших языковых моделей»

#### **1. Проблема интерпретируемости**
- **Полисемантика нейронов**:
  $$\text{Нейрон} = \sum_{i} \alpha_i \cdot \text{Признак}_i$$
  Пример: активация на "золотые ворота" и "ошибки в коде"
- **Решение**: Разреженные автоэнкодеры (SAE)

#### **2. Разреженные автоэнкодеры (SAE)**
- **Архитектура**:


- **Функция потерь**:
  $$\mathcal{L} = \underbrace{\|x - W_{dec}y\|_2}_{\text{Реконструкция}} + \underbrace{\lambda\|y\|_1}_{\text{Разреженность}}$$
  где $y = \text{ReLU}(W_{enc}x + b_{enc})$

#### **3. Feature Steering**
- **Управление поведением LLM**:
  1. Идентификация признака (напр. "честность")
  2. Усиление активаций:
     $$\tilde{y} = y + \alpha \cdot y_{\text{feature}}$$
  3. Декодирование:
     $$\hat{x} = W_{dec}\tilde{y}$$

#### **4. Трансформерные цепи**
- **Residual Stream**:
  $$h_{l+1} = h_l + \text{Attn}(h_l) + \text{MLP}(h_l)$$
- **QK/VO цепи**:
  $$\text{Attention} = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
  - QK: что копировать
  - VO: куда копировать

#### **5. Примеры цепей**
1. **Индукционные головы**:
   - Паттерн: "A B ... A → B"
   - Механизм:
     ```mermaid
     graph LR
         A[Токен A] -->|QK| B[Токен B]
         B -->|VO| C[Предсказание B]
     ```

2. **Модульное сложение**:
   - Числа → Круговое пространство:
     $$x \mod p$$

#### **6. Применение**
- **Поиск признаков**:

#### **7. Ключевые формулы**
- **Реконструкция SAE**:
  $$\hat{x} = \sum_{j=1}^m y_j W_{dec}^{(j)}$$
- **Разреженность**:
  $$\text{sparsity} = \frac{\|y\|_0}{m}$$

### Лекция «Данные и ML-модели»