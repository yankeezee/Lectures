![[Pasted image 20250701072737.png]]
### **Бенчмарк** (от англ. _benchmark_) 
— это стандартный тест или набор задач для объективной оценки производительности моделей ИИ (например, языковых моделей, как GPT или DeepSeek).

### **End-to-end модели (или сквозные модели)**
— это такие модели в машинном обучении, которые обрабатывают данные от начального этапа до конечного результата без промежуточных этапов или ручного проектирования признаков.

### **Supervised fine-tuning (SFT)**
— это методика, применяемая для адаптации предварительно обученных Large Language Model (LLM) под конкретную задачу при помощи размеченных данных. https://habr.com/ru/articles/829318/

### **RLHF (Reinforcement Learning from Human Feedback)** 
- это метод обучения языковой модели, который позволяет ей научиться соответствовать ожиданиям людей. Сначала выучивается _модель вознаграждения_, которая по тексту дает оценку, насколько ответ подходит человеку. Затем она используется для обучения языковой модели с помощью _метода обучения с подкреплением (RL)_. В результате модель научится генерировать более соответствующие человеческим ожиданиям ответы на заданные тексты. https://habr.com/ru/articles/730990/

### **Monte Carlo Tree Search (MCTS)** 
— это алгоритм для принятия решений в сложных средах (например, игры, планирование задач) через **адаптивный поиск по дереву возможных действий**.  

### **Что такое SOTA?**
— **Лучший из существующих** на момент публикации метод в определённой области (например, sample-efficient RL).  
— Benchmark, с которым сравнивают новые алгоритмы.

### **Self-attention (Самовнимание)**

**Что это?**  
Механизм, позволяющий модели анализировать взаимосвязи между всеми элементами входной последовательности (например, словами в предложении).

**Как работает?**

- Для каждого токена (слова) вычисляются три вектора:
    
    - **Query (Q)** — что ищем.
        
    - **Key (K)** — что можем предложить.
        
    - **Value (V)** — информация для передачи.
        
- Веса внимания: определяют, насколько токен ii зависит от токена jj:
    
    Attention(Q,K,V)=softmax(QKTd)VAttention(Q,K,V)=softmax(d​QKT​)V
- **Пример:** В предложении _"Кот ест рыбу"_ слово _"ест"_ уделяет больше внимания _"коту"_ и _"рыбе"_.
    

**Зачем нужно?**  
Позволяет модели учитывать контекстные зависимости любой длины (в отличие от RNN, где информация передаётся последовательно).

**Model Context Protocol (MCP)** — это открытый стандарт, регулирующий способ предоставления контекста и данных AI-моделям. По сути, MCP служит «универсальным разъемом» между LLM - приложением (агентом) и внешними системами.

### Аугментация
В машинном обучении и компьютерном зрении - это процесс создания новых тренировочных данных из существующих путем добавления небольших изменений или искажений к исходным данным. Это помогает улучшить производительность моделей машинного обучения, делая их более устойчивыми к различным вариациям входных данных

### **Конкатенация** 
– это операция соединения двух или более объектов в один, обычно применяется к строкам, спискам, массивам и другим структурам данных. В контексте строк, конкатенация означает добавление одной строки в конец другой.