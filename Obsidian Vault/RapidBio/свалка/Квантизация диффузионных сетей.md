### Что такое квантизация? (Базовое понятие)

Представьте, что у вас есть весы с очень высокой точностью, которые могут взвесить яблоко с точностью до микрограмма. Но для продажи в магазине вам достаточно точности до грамма. **Квантизация** — это как переход от сверхточных весов к магазинным.

В мире нейросетей:
*   Модели изначально обучаются с использованием чисел **высокой точности** (обычно 32-битные числа с плавающей запятой, `float32`).
*   **Квантизация** — это процесс преобразования этих чисел в числа **меньшей точности** (например, 16-битные `float16`, 8-битные целые числа `int8` и даже 4-битные `int4`).

### Зачем квантизировать диффузионные сети?

Диффузионные модели (как Stable Diffusion, DALL-E) — это очень большие и сложные сети. Их преимущества становятся проблемой при развертывании:

1.  **Большой объем памяти:** Модель может занимать гигабайты дискового пространства. Загрузка ее в оперативную память (ОЗУ) для inference (генерации изображений) требует еще больше ресурсов.
2.  **Высокие вычислительные затраты:** Операции с `float32` требуют много энергии и времени на CPU и, что важно, не всегда оптимально используют возможности GPU.
3.  **Медленная генерация:** Один из главных минусов — долгое ожидание картинки.

**Квантизация решает эти проблемы:**
*   **Сокращение размера модели:** Переход с `float32` на `int8` уменьшает размер модели в ~4 раза. Переход на `int4` — в ~8 раз.
*   **Ускорение вычислений:** Многие процессоры (особенно мобильные и некоторые GPU) гораздо быстрее работают с целыми числами низкой разрядности.
*   **Снижение потребления памяти:** Меньший объем модели означает меньшую нагрузку на ОЗУ и VRAM, что позволяет запускать мощные модели на более доступном железе (например, на видеокартах с 6-8 ГБ VRAM).

**Главный компромисс:** Между **скоростью/размером** и **качеством**. Слишком агрессивная квантизация может привести к появлению артефактов на изображениях, потере деталей или "мусору" на картинках.

---

### Основные методы квантизации

Методы можно разделить по двум критериям: *когда* она происходит и *как*.

#### 1. По времени выполнения:

**а) Пост-тренировочная квантизация (Post-Training Quantization, PTQ)**
Это самый популярный и простой подход.

*   **Как работает:**
    1.  Берется уже **обученная** модель в `float32`.
    2.  Подается небольшой калибровочный датасет (например, 128-512 изображений).
    3.  Для каждого тензора (весов модели) анализируются его значения на этом датасете, чтобы найти минимальное и максимальное значение. Это нужно, чтобы "натянуть" диапазон `float32` на диапазон `int8`.
    4.  По найденным масштабам и смещениям веса преобразуются в низкоразрядный формат.

*   **Формула (линейная квантизация):**
    `x_quantized = round( (x_float - zero_point) / scale )`

    Где `scale` — масштабный коэффициент, `zero_point` — смещение, чтобы представить ноль.

*   **Плюсы:**
    *   Быстро и просто. Не требует переобучения модели.
    *   Широко поддерживается фреймворками (ONNX Runtime, TensorRT, PyTorch).
*   **Минусы:**
    *   Может привести к заметной потере качества, особенно на сложных задачах.

**б) Квантизация во время обучения (Quantization-Aware Training, QAT)**
Более сложный, но и более качественный метод.

*   **Как работает:**
    1.  Берется предобученная `float32` модель.
    2.  В ее граф вычислений **встраиваются "фальшивые" (fake) операторы квантизации**. Они имитируют эффект низкой разрядности во время прямого прохода (округление, масштабирование), но при этом градиенты во время обратного прохода вычисляются как обычно (используется трюк "straight-through estimator").
    3.  Модель **дообучается** (fine-tune) на нескольких эпохах с этими операторами. Это позволяет модели **адаптироваться** к ошибкам, вносимым квантизацией, и найти веса, которые будут работать хорошо уже в низком формате.

*   **Плюсы:**
    *   Значительно лучше сохраняет качество по сравнению с PTQ, особенно при агрессивной квантизации (например, до `int8` или `int4`).
*   **Минусы:**
    *   Требует вычислительных ресурсов и времени для дообучения.
    *   Более сложен в реализации.

---

#### 2. По granularity (уровню детализации):

Это отвечает на вопрос: "Для какого блока весов мы вычисляем свои параметры масштаба и смещения?".

*   **Per-Tensor (На весь тензор):** Самый простой метод. Для всего большого тензора весов (например, всей матрицы свертки) вычисляется один `scale` и один `zero_point`. Эффективно, но не очень точно, если значения в тензоре сильно различаются.
*   **Per-Channel (На канал):** Особенно полезно для весов сверточных слоев. Для *каждого выходного канала* вычисляются свои `scale` и `zero_point`. Это гораздо точнее, так как учитывает, что разные фильтры (каналы) могут иметь разный диапазон значений. Это стандарт для современных PTQ-методов.
*   **Per-Token/Per-Activation (На активацию):** Применяется не только к весам, но и к активациям (выходам слоев). Для каждого входного примера или даже для каждого токена в последовательности активации могут иметь разный динамический диапазон. Учет этого повышает точность, но усложняет вычисления.

---

### Практическое применение в диффузионных моделях

Давайте рассмотрим, как это выглядит на практике, например, для Stable Diffusion.

1.  **Цель:** Запустить SD 1.5 на видеокарте с 4 ГБ VRAM и генерировать картинки быстрее.

2.  **Выбор метода:**
    *   **Простейший вариант (PTQ):** Использовать встроенные возможности библиотек. Например, в `diffusers` можно легко загрузить модель в `float16` или применить встроенные квантизаторы. Многие готовые оптимизированные модели (например, для ONNX или TensorRT) уже используют PTQ.
    *   **Продвинутый вариант (QAT):** Если качество после PTQ не устраивает, исследователи проводят QAT. Например, можно заморозить VAE и текстовый энкодер, а квантизацией с дообучением подвергнуть только U-Net — самую тяжелую часть модели.

3.  **Популярные инструменты и форматы:**
    *   **ONNX Runtime:** Поддерживает PTQ (Static Quantization) для диффузионных моделей. Многие готовые квантизированные версии моделей экспортируются в ONNX.
    *   **TensorRT от NVIDIA:** Использует сложные методы PTQ (часто с калибровкой) и компилирует модель в высокооптимизированный движок, который может работать в `int8`.
    *   **GGML/GGUF (для CPU):** Формат, используемый в таких проектах, как `llama.cpp`. Позволяет запускать диффузионные модели на CPU, поддерживая различные типы квантизации (`q4_0`, `q5_1`, `q8_0` и т.д.).
    *   **PyTorch:** Имеет встроенные модули для проведения как PTQ, так и QAT (`torch.ao.quantization`).

### Итог

| Метод | Сложность | Качество | Скорость | Использование |
| :--- | :--- | :--- | :--- | :--- |
| **PTQ** | Низкая | Хорошее (может ухудшиться) | Высокая | Быстрое прототипирование, готовые решения |
| **QAT** | Высокая | Отличное | Высокая | Критичные к качеству продакшн-системы |

**Квантизация диффузионных сетей** — это не магия, а мощный и необходимый инженерный метод, который делает передовые AI-модели доступными для широкого круга пользователей и устройств. Он является ключевым звеном в цепочке от исследования к практическому применению.