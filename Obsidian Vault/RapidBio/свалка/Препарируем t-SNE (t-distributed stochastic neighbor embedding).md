### I. Введение и общие сведения

*   **t-SNE** (t-distributed stochastic neighbor embedding) — техника нелинейного снижения размерности и визуализации многомерных данных.
*   **Авторы:** Лоуренс ван дер Маатен (Laurens van der Maaten) и Джеффри Хинтон (Geoffrey Hinton).
*   **Публикация:** 2008 год (ссылка 1).
*   **Предшественник:** **SNE** (Stochastic Neighbor Embedding), предложенный Хинтоном и Ровейсом в 2002 году.
*   **Основное отличие t-SNE от SNE:** Замена нормального распределения (Гауссова) на **t-распределение Стьюдента** для данных низкой размерности (в пространстве отображения). Это стало одним из «трюков», которые упростили поиск глобальных минимумов и повысили качество визуализации.

### II. Математический аппарат SNE (Stochastic Neighbor Embedding)

#### 1. Постановка задачи
Получить новую, низкоразмерную переменную $Y$ (2D или 3D), которая максимально сохраняет структуру и закономерности исходных многомерных данных $X$.

#### 2. Преобразование сходства (Вероятности $p_{j|i}$)
Евклидова дистанция между точками $X_i$ и $X_j$ в многомерном пространстве преобразуется в **условные вероятности $p_{j|i}$** (сходство точек) с использованием Гауссова распределения вокруг $X_i$:

$$p_{j|i} = \frac{\exp(-\|x_i - x_j\|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-\|x_i - x_k\|^2 / 2\sigma_i^2)} \quad \text{(Формула 1)}$$

*   **$\sigma_i$ (отклонение):** Различно для каждой точки $X_i$. Выбирается так, чтобы точки в областях с большей плотностью имели меньшую дисперсию.

#### 3. Перплексия ($Perp$)
Параметр, используемый для определения $\sigma_i$.
*   **Формула:** $Perp(P_i) = 2^{H(P_i)}$, где $H(P_i)$ — энтропия Шеннона: $H(P_i) = -\sum_j p_{j|i} \log_2 p_{j|i}$ (Формула 2).
* **perplexity** — мера [неопределенности](https://en.wikipedia.org/wiki/Uncertainty "Uncertainty") в значении [выборки](https://en.wikipedia.org/wiki/Sample_\(statistics\) "Sample (statistics)") из [дискретного распределения вероятностей](https://en.wikipedia.org/wiki/Discrete_probability_distribution "Discrete probability distribution"). Чем больше недоумение, тем меньше вероятность того, что наблюдатель сможет угадать значение, которое будет получено из распределения. Термин «Недоумение» был впервые введен в 1977 году в контексте [распознавания речи](https://en.wikipedia.org/wiki/Speech_recognition "Speech recognition") [Фредериком Елинеком](https://en.wikipedia.org/wiki/Frederick_Jelinek "Frederick Jelinek"), [Робертом Лероем Мерсером](https://en.wikipedia.org/wiki/Robert_Mercer "Robert Mercer"), [Лалитом Р. Балом и Джеймсом К. Бейкером](https://en.wikipedia.org/wiki/James_K._Baker "James K. Baker"). [[1]](https://en.wikipedia.org/wiki/Perplexity#cite_note-1)
*   **Интерпретация:** Сглаженная оценка эффективного количества «соседей» для точки $X_i$.
	Перплексия — это **параметр** алгоритма t-SNE, который задает, **сколько точек** (соседей) алгоритм должен считать **важными** для формирования локальной структуры вокруг данной точки $X_i$.
	
	*   Это не простое количество, а **сглаженная, взвешенная оценка** (основанная на энтропии).
	*   Фактически, перплексия **контролирует дисперсию ($\sigma_i$)** Гауссова распределения, используемого для расчета сходства $p_{j|i}$ в многомерном пространстве.
	*   Чем выше перплексия, тем **шире окрестность** (больше $\sigma_i$) учитывается.
*   **Рекомендация:** Значение в интервале от 5 до 50. $\sigma_i$ определяется для каждой пары $(X_i, X_j)$ при помощи **алгоритма бинарного поиска**.

#### 4. Условная вероятность в пространстве отображения ($q_{j|i}$)
Аналогичная формула, но в низкоразмерном пространстве $Y$, со стандартным отклонением $\sigma$ (предлагается $\sigma = 1/\sqrt{2}$):
$$q_{j|i} = \frac{\exp(-\|y_i - y_j\|^2)}{\sum_{k \neq i} \exp(-\|y_i - y_k\|^2)} \quad \text{(Модифицированная Формула 1)}$$

#### 5. Функция потерь (Cost Function)
Минимизация суммы **дивергенций Кульбака-Лейблера (KL)** для всех точек отображения при помощи градиентного спуска:
$$Cost = \sum_i KL(P_i || Q_i) = \sum_i \sum_j p_{j|i} \log \frac{p_{j|i}}{q_{j|i}} \quad \text{(Формула 3)}$$

*   **Градиент:** Градиент $\frac{\partial Cost}{\partial y_i}$ имеет относительно простой вид
![[Pasted image 20251017232547.png]]
*   **Оптимизация:** Используется с учетом **моментов** (инерции), что соответствует физической аналогии системы, приходящей в равновесие.
*   **Проблемы SNE:** Трудности в оптимизации функции потерь и **проблема скученности (crowding problem)**.
	* **Проблема скученности (Crowding Problem):**
    
    - **Что это:** Неспособность 2D/3D пространства отображения "вместить" все точки, которые являются **среднеудаленными** в исходном многомерном пространстве.
        
    - **Почему возникает (в SNE):** Гауссово распределение (используемое в SNE для низкоразмерных данных) имеет **слишком "тонкие" хвосты**, что требует очень большого расстояния между точками в 2D/3D, чтобы смоделировать среднеудаленные отношения из многомерного пространства.
        
    - **Результат:** Кластеры в 2D/3D получаются **слишком сжатыми** и плохо разделенными, потому что нет "места" для размещения всех промежуточных точек. (t-SNE решает это, используя t-распределение с **"тяжелыми хвостами"**).

### III. Улучшения в t-SNE

t-SNE существенно облегчает оптимизацию благодаря двум принципиальным отличиям:

#### 1. Симметричная форма сходства
Вместо условных вероятностей $p_{j|i}$ используется **совместная вероятность $P_{ij}$** в многомерном пространстве:
$$P_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}$$
где $n$ — количество точек.

*   **Новая Функция потерь:** Минимизация одиночной KL-дивергенции между совместными вероятностями $P$ (высокоразмерное) и $Q$ (низкоразмерное):
$$Cost = KL(P || Q) = \sum_i \sum_j P_{ij} \log \frac{P_{ij}}{Q_{ij}}$$

#### 2. Использование t-распределения Стьюдента
Для точек в пространстве отображения $Y$ (низкоразмерном) используется **t-распределение с одной степенью свободы** (Cauchy distribution):
$$q_{ij} = \frac{(1 + \|y_i - y_j\|^2)^{-1}}{\sum_{k \neq l} (1 + \|y_k - y_l\|^2)^{-1}} \quad \text{(Формула 4)}$$
*   **Свойство:** **«Тяжелые» хвосты** t-распределения значительно облегчают оптимизацию и решают **проблему скученности**.

#### 3. Градиент t-SNE
$$ \frac{\partial Cost}{\partial y_i} = 4 \sum_j (P_{ij} - q_{ij}) (y_i - y_j) (1 + \|y_i - y_j\|^2)^{-1} \quad \text{(Формула 5)} $$
*   **Смысл:** Результирующая сила стягивает точки пространства отображения для близлежащих точек многомерного пространства и отталкивает — для удаленных.

### IV. Алгоритм и Трюки Оптимизации

#### 1. Упрощенный псевдокод

1.  Вычислить попарное сходство $p_{j|i}$ с перплексией $Perp$.
2.  Установить симметричное сходство $P_{ij} = (p_{j|i} + p_{i|j}) / 2n$.
3.  Инициализировать $Y(0)$ точками нормального распределения.
4.  Цикл $t=1$ до $T$:
    *   Вычислить сходство $Q_{ij}$ (Формула 4).
    *   Вычислить градиент $\frac{\partial Cost}{\partial Y}$ (Формула 5).
    *   Обновить $Y(t)$ с учетом скорости обучения $\eta$ и момента $\alpha(t)$.

#### 2. Трюки для улучшения результата
*   **Ранняя компрессия (Early Compression):** В начале оптимизации заставить точки быть ближе друг к другу (за счет добавочного L2-штрафа), чтобы кластеры легче перемещались в поисках глобальных минимумов.
*   **Раннее гиперусиление (Early Exaggeration):** В начале оптимизации (например, первые 50 итераций) умножить все $P_{ij}$ на целое число (например, 4). Смысл: для больших $P_{ij}$ получить большие $Q_{ij}$, что создает **плотные и широко разнесенные кластеры**.

### V. Реализация на R

*   **Исходный код:** Портирован с оригинальной MatLab реализации ван дер Маатена.
*   **Рекомендации по реализации:**
    *   Для расчета матрицы квадратов евклидовой дистанции лучше использовать функцию `rdist()` из пакета `fields` (работает быстрее `dist()` из `stats`).
    *   Инициализация пространства отображения $Y$ с помощью `rnorm()` (нормальное распределение) с нулевым мат. ожиданием и $\sigma = 1e-4$.
*   **Расчет логарифма перплексии:** Используется натуральный логарифм. $\sigma_i$ определяется бинарным поиском, итерация которого ограничена 50 попытками.
*   **Адаптивная скорость обучения:** Для поиска оптимальных значений $Y$ используется эвристика Роберта Джекобса **«дельта-бар-дельта»** для адаптивного изменения шага обучения (Learning Rate $\eta$).

### VI. Эксперименты и Выводы

#### 1. Визуализация MNIST
*   **Задача:** Визуализация структуры кластеров 60 000 изображений рукописных цифр.
*   **Эксперимент:** Использовано 6000 случайных изображений. 1000 итераций заняла ~1.5 часа.
*   **Ключевой совет авторов:** **Предварительная обработка данных с помощью PCA** (анализ главных компонент), выбор **первых 30 компонент**, существенно улучшает результат.

#### 2. Проблема масштабирования
*   **Скорость:** Алгоритм пропорционален **квадрату количества точек** ($O(N^2)$). Например, 2000 точек обрабатываются за ~9 минут.
*   **Решение для больших данных:** Использовать специальную реализацию, основанную на **деревьях ближайших соседей** и методе **случайного блуждания** (например, пакет **`Rtsne`** в R, реализующий **Barnes-Hut-SNE**), которая имеет сложность $O(N \log N)$ или $O(N)$. `Rtsne` может обработать 60 000 MNIST менее чем за час.

#### 3. Распределенное представление слов (Word2Vec)
*   **Проблема:** В реальных данных (например, Word2Vec) часто наблюдается большое количество кластеров и существенные различия по их размеру.
*   **Проблема скученности:** Алгоритм может объединять равноудаленные точки в один кластер с большей вероятностью, если между ними есть другие точки (пример с точками А, В и С).
*   **Рекомендация для визуализации:** Вместо случайной выборки лучше **отобрать интересные кластеры** (например, с помощью k-means или встроенной кластеризации Word2Vec) для наглядности.