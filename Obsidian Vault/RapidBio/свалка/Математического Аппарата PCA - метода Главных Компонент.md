### Объяснение Математического Аппарата PCA

Метод PCA (Principal Component Analysis) — это, по сути, поиск нового ортогонального (перпендикулярного) базиса (набора осей), в котором наши данные будут иметь **наибольшую дисперсию** (наибольший разброс).

Новые оси (главные компоненты) должны быть расположены так, чтобы, спроецировав на них данные, мы потеряли минимальное количество информации.

#### Шаг 1: Подготовка Данных (Центрирование)

*   **Цель:** Упростить вычисления и гарантировать, что первая главная компонента пройдет через «центр тяжести» нашего облака данных.
*   **Действие:** Мы вычитаем **среднее арифметическое (Математическое Ожидание, $E(X)$ или $\mu$)** из каждого признака.
*   **Математический результат:** Центрированные данные имеют нулевое среднее: $E(X_{centered}) = 0$.

> **Интуиция:** Если данные центрированы, математические формулы для дисперсии и ковариации сильно упрощаются, так как нам не нужно постоянно вычитать среднее значение.

#### Шаг 2: Ковариационная Матрица ($\Sigma$)

Для одномерного случая мы используем **Дисперсию** ($Var(X)$) для измерения разброса. Для многомерного случая нам нужно обобщение — **Ковариационная Матрица ($\Sigma$)**. Она описывает **форму, размер и ориентацию** многомерного облака данных (аппроксимирующего эллипсоида).

**Ковариация** (корреляционный момент) — **мера зависимости двух случайных величин** в теории вероятностей и математической статистике. Показывает, в какой степени переменные изменяются вместе, и указывает направление линейной зависимости. [ru.ruwiki.ru](https://ru.ruwiki.ru/wiki/%D0%9A%D0%BE%D0%B2%D0%B0%D1%80%D0%B8%D0%B0%D1%86%D0%B8%D1%8F)[ru.wikipedia.org*](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D0%B2%D0%B0%D1%80%D0%B8%D0%B0%D1%86%D0%B8%D1%8F)[wiki.loginom.ru](https://wiki.loginom.ru/articles/covariation.html)[nuancesprog.ru](https://nuancesprog.ru/p/3332/)[dzen.ru](https://dzen.ru/a/YeQg_OfO_ivX6JZm)[geeksforgeeks.org](https://www.geeksforgeeks.org/data-science/covariance-vs-correlation-understanding-differences-and-applications/)


Ковариация — это ожидаемое значение (или среднее) произведения отклонений двух случайных величин от их индивидуальных ожидаемых значений. [en.wikipedia.org](https://tr-page.yandex.ru/translate?lang=en-ru&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FCovariance)
##### Структура Матрицы

Это симметричная матрица, где:
1.  **Диагональные элементы ($\Sigma_{i,i}$):** Это **Дисперсии** $Var(X_i)$ отдельных признаков. Они показывают разброс данных вдоль каждой *исходной* оси.
    $$\Sigma_{i,i} = Var(X_i) = \frac{1}{n-1}\sum_{k=1}^{n} (X_{i,k} - E(X_i))^2$$
2.  **Недиагональные элементы ($\Sigma_{i,j}$):** Это **Ковариации** $Cov(X_i, X_j)$ между парами признаков. Они показывают степень их линейной зависимости.
    $$\Sigma_{i,j} = Cov(X_i, X_j) = \frac{1}{n-1}\sum_{k=1}^{n} (X_{i,k} - E(X_i))(X_{j,k} - E(X_j))$$

##### Упрощение с Центрированными Данными

Поскольку мы центрировали данные, $E(X_i) = 0$ и $E(X_j) = 0$. Формула ковариации, которую вы привели в тексте, упрощается:

$$\Sigma_{i,j} = Cov(X_i, X_j) \approx \frac{1}{n-1} \sum_{k=1}^{n} X_{i,k} \cdot X_{j,k}$$

> **Интуиция:** Ковариационная матрица является "картой" связей и разброса в данных. Если ковариация между $X$ и $Y$ большая и положительная, это значит, что при увеличении $X$ увеличивается и $Y$, и наше облако данных вытянуто (например, под углом $45^\circ$).

#### Шаг 3: Собственные Векторы и Собственные Значения (Айгенпары)

Теперь, когда у нас есть матрица $\Sigma$, описывающая форму облака, нам нужно найти направления (векторы), вдоль которых эта форма максимально вытянута.

*   **Задача:** Найти вектор $v$ (направление), который максимизирует дисперсию проекции: $Var(v^T X)$. В векторной форме эта дисперсия равна **$v^T \Sigma v$**.
*   **Решение:** Ответ на эту задачу дает **Уравнение на Собственные Значения** (Eigenvalue Equation):
    $$\Sigma v = \lambda v$$

Здесь:
*   **$v$ (Собственный Вектор / Eigenvector):** Это и есть **Главная Компонента (Principal Component)**. Он указывает направление в пространстве, вдоль которого дисперсия данных максимальна (для первой компоненты), а затем максимальна в ортогональных направлениях (для последующих).
*   **$\lambda$ (Собственное Значение / Eigenvalue):** Это величина, которая численно равна **Дисперсии** данных, спроецированных на соответствующий собственный вектор $v$.

> **Интуиция:** Собственные векторы — это полуоси эллипсоида, аппроксимирующего наше облако данных. Собственные значения — это "длины" этих полуосей, то есть величина разброса (дисперсии) вдоль этих новых осей.

#### Шаг 4: Снижение Размерности (Проекция)

1.  **Ранжирование:** Мы сортируем собственные значения ($\lambda$) по убыванию.
2.  **Выбор:** Мы выбираем $k$ собственных векторов ($v$) с **самыми большими** $\lambda$, так как они сохраняют наибольшую долю информации (дисперсии).
3.  **Оценка потерь/сохранения:** Доля сохраненной информации для компоненты $i$ рассчитывается как:
    $$\frac{\lambda_i}{\sum_{j=1}^{n} \lambda_j}$$
    Сумма всех собственных значений равна сумме дисперсий по исходным осям (это $\Sigma_{i,i}$), то есть это общая информация о разбросе.
4.  **Проекция:** Для снижения размерности мы проецируем центрированные данные $X_{centered}$ на выбранные $k$ собственных векторов (матрицу $V$):
    $$X_{new} = V^T X_{centered}$$

$X_{new}$ — это наш редуцированный набор данных с $k$ новыми, некоррелированными признаками (Главными Компонентами).

---

### Резюме

PCA находит **оптимальное вращение** осей (базиса) таким образом, чтобы дисперсия данных была сосредоточена на как можно меньшем количестве новых осей (Главных Компонентах), позволяя нам безопасно отбросить наименее информативные направления.